{"pages":[],"posts":[{"title":"Pytorch1","text":"Gradient Descentexercise) $y=x^2w_2+xw_1+b-y$Using pytorch12345import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimtorch.manual_seed(1) &lt;torch._C.Generator at 0x2cdbae1d070&gt; 12345678x_train = torch.FloatTensor([[1.0],[2.0],[3.0]])y_train = torch.FloatTensor([[2.0],[4.0],[6.0]])w1 = torch.zeros(1, requires_grad=True)w2 = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True)def forward(x): return x*x*w2 + x*w1 + bprint(forward(4)) tensor([0.], grad_fn=&lt;AddBackward0&gt;) 123456789101112131415161718192021# optimizer 설정optimizer = optim.SGD([w2,w1,b], lr=2*1e-2)nb_epochs = 2000for epoch in range(nb_epochs + 1): # H(x) 계산 hypothesis = x_train*x_train*w2+x_train*w1+b # cost 계산 cost = torch.mean((hypothesis - y_train) ** 2) # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() if epoch % 100 == 0: print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} b: {:.3f} Cost: {:.6f}'.format( epoch, nb_epochs, w1.item(), w2.item(), b.item(), cost.item() )) Epoch 0/2000 w1: 0.373 w2: 0.960 b: 0.160 Cost: 18.666666 Epoch 100/2000 w1: 0.858 w2: 0.302 b: 0.828 Cost: 0.025777 Epoch 200/2000 w1: 0.959 w2: 0.255 b: 0.891 Cost: 0.014748 Epoch 300/2000 w1: 0.998 w2: 0.243 b: 0.875 Cost: 0.013746 Epoch 400/2000 w1: 1.029 w2: 0.235 b: 0.850 Cost: 0.012942 Epoch 500/2000 w1: 1.058 w2: 0.228 b: 0.825 Cost: 0.012187 Epoch 600/2000 w1: 1.086 w2: 0.221 b: 0.801 Cost: 0.011475 Epoch 700/2000 w1: 1.113 w2: 0.215 b: 0.777 Cost: 0.010806 Epoch 800/2000 w1: 1.139 w2: 0.209 b: 0.754 Cost: 0.010175 Epoch 900/2000 w1: 1.165 w2: 0.202 b: 0.732 Cost: 0.009581 Epoch 1000/2000 w1: 1.189 w2: 0.196 b: 0.710 Cost: 0.009022 Epoch 1100/2000 w1: 1.214 w2: 0.191 b: 0.689 Cost: 0.008495 Epoch 1200/2000 w1: 1.237 w2: 0.185 b: 0.669 Cost: 0.008000 Epoch 1300/2000 w1: 1.259 w2: 0.179 b: 0.649 Cost: 0.007533 Epoch 1400/2000 w1: 1.281 w2: 0.174 b: 0.630 Cost: 0.007093 Epoch 1500/2000 w1: 1.303 w2: 0.169 b: 0.611 Cost: 0.006679 Epoch 1600/2000 w1: 1.323 w2: 0.164 b: 0.593 Cost: 0.006289 Epoch 1700/2000 w1: 1.343 w2: 0.159 b: 0.575 Cost: 0.005922 Epoch 1800/2000 w1: 1.363 w2: 0.154 b: 0.558 Cost: 0.005577 Epoch 1900/2000 w1: 1.382 w2: 0.150 b: 0.542 Cost: 0.005251 Epoch 2000/2000 w1: 1.400 w2: 0.145 b: 0.526 Cost: 0.004945 1print(forward(4)) tensor([8.4512], grad_fn=&lt;AddBackward0&gt;) Manual참조 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# Training Datax_data = [1.0, 2.0, 3.0]y_data = [2.0, 4.0, 6.0]# a random guess: random valuew1 = 0w2 = 0b = 0# our model forward passdef forward(x): return pow(x,2)*w2 + x*w1 + b# Loss functiondef loss(x, y): y_pred = forward(x) return (y_pred - y) * (y_pred - y)# compute gradientdef w1_gradient(x, y, y_pred): # d_loss/d_w1 return 2 * x * (y_pred - y)def w2_gradient(x, y, y_pred): # d_loss/d_w2 return 2 * pow(x, 2) * (y_pred - y)def b_gradient(x, y, y_pred): # d_loss/d_b return 2 * (y_pred - y)# Update the weights and the biasdef optimize(x, y, learning_rate = 0.02): global w1, w2, b y_pred = forward(x) w1_grad = w1_gradient(x, y, y_pred) w1 = w1 - learning_rate * w1_grad w2_grad = w2_gradient(x, y, y_pred) w2 = w2 - learning_rate * w2_grad b_grad = b_gradient(x, y, y_pred) b = b - learning_rate * b_grad# print('\\tgrad(w1, w2, b): ', x_val, y_val, round(w1_grad, 2), round(w2_grad, 2), round(b_grad, 2)) return w1, w2, b# Before trainingprint(&quot;Prediction (before training)&quot;, 4, forward(4))# Training loopfor epoch in range(20): for x_val, y_val in zip(x_data, y_data): # Compute derivative w.r.t to the learned weights # Compute the loss and print progress w1, w2, b = optimize(x_val, y_val) l = loss(x_val, y_val) print(&quot;progress : &quot;, epoch, &quot;loss = &quot;, round(l, 2))# After trainingprint(&quot;Predicted score (after training)&quot;, &quot;4 hours of studying: &quot;, forward(4)) Prediction (before training) 4 0 progress : 0 loss = 6.38 progress : 1 loss = 11.8 progress : 2 loss = 6.54 progress : 3 loss = 6.22 progress : 4 loss = 4.57 progress : 5 loss = 3.81 progress : 6 loss = 3.01 progress : 7 loss = 2.45 progress : 8 loss = 1.98 progress : 9 loss = 1.61 progress : 10 loss = 1.32 progress : 11 loss = 1.08 progress : 12 loss = 0.9 progress : 13 loss = 0.75 progress : 14 loss = 0.63 progress : 15 loss = 0.53 progress : 16 loss = 0.45 progress : 17 loss = 0.38 progress : 18 loss = 0.33 progress : 19 loss = 0.29 Predicted score (after training) 4 hours of studying: 7.71975178478036 Optimizer참조 Logistic RegressionBinary Classification using sigmoid12345%matplotlib inlineimport numpy as np # 넘파이 사용import matplotlib.pyplot as plt # 맷플롯립사용def sigmoid(x): # 시그모이드 함수 정의 return 1/(1+np.exp(-x)) 1234567891011x = np.arange(-5.0, 5.0, 0.1)y1 = sigmoid(0.5*x)y2 = sigmoid(x)y3 = sigmoid(2*x)plt.plot(x, y1, 'r', linestyle='--') # W의 값이 0.5일때plt.plot(x, y2, 'g') # W의 값이 1일때plt.plot(x, y3, 'b', linestyle='--') # W의 값이 2일때plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가plt.title('Sigmoid Function')plt.show() ​​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from torch import tensorfrom torch import nnfrom torch import sigmoidimport torch.nn.functional as Fimport torch.optim as optim# Training data and ground truthx_data = tensor([[1.0], [2.0], [3.0], [4.0]])y_data = tensor([[0.], [0.], [1.], [1.]])class Model(nn.Module): def __init__(self): &quot;&quot;&quot; In the constructor we instantiate nn.Linear module &quot;&quot;&quot; super(Model, self).__init__() self.linear = nn.Linear(1, 1) # One in and one out def forward(self, x): &quot;&quot;&quot; In the forward function we accept a Variable of input data and we must return a Variable of output data. &quot;&quot;&quot; y_pred = sigmoid(self.linear(x)) return y_pred# our modelmodel = Model()# Construct our loss function and an Optimizer. The call to model.parameters()# in the SGD constructor will contain the learnable parameters of the two# nn.Linear modules which are members of the model.criterion = nn.BCELoss(reduction='mean')optimizer = optim.SGD(model.parameters(), lr=0.01)# Training loopfor epoch in range(1000): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x_data) # Compute and print loss loss = criterion(y_pred, y_data) if epoch%100==0: print(f'Epoch {epoch}/1000 | Loss: {loss.item():.4f}') # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step()# After trainingprint(f'\\nLet\\'s predict the hours need to score above 50%\\n{&quot;=&quot; * 50}')hour_var = model(tensor([[1.0]]))print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() &gt; 0.5}')hour_var = model(tensor([[7.0]]))print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() &gt; 0.5}') Epoch 0/1000 | Loss: 0.5570 Epoch 100/1000 | Loss: 0.5298 Epoch 200/1000 | Loss: 0.5114 Epoch 300/1000 | Loss: 0.4944 Epoch 400/1000 | Loss: 0.4786 Epoch 500/1000 | Loss: 0.4638 Epoch 600/1000 | Loss: 0.4499 Epoch 700/1000 | Loss: 0.4368 Epoch 800/1000 | Loss: 0.4246 Epoch 900/1000 | Loss: 0.4131 Let's predict the hours need to score above 50% ================================================== Prediction after 1 hour of training: 0.3149 | Above 50%: False Prediction after 7 hours of training: 0.9854 | Above 50%: True Activation Functions 1","link":"/2021/01/25/Pytorch/Pytorch1/"},{"title":"Day11","text":"베이즈 통계학 History Pytorchs Kaiming initialization https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138","link":"/2021/02/01/BoostCamp/Day11/"}],"tags":[],"categories":[{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"BoostCamp","slug":"AI/BoostCamp","link":"/categories/AI/BoostCamp/"},{"name":"Pytorch","slug":"AI/Pytorch","link":"/categories/AI/Pytorch/"}]}