{"pages":[],"posts":[{"title":"Hello","text":"HelloThis is","link":"/2021/01/18/Hello/"},{"title":"Pytorch1","text":"Gradient Descentmulti-variable linear regression$y=x^2w_2+xw_1+b-y$ \\begin{pmatrix} 1 & a_1 & a_1^2 & \\cdots & a_1^n \\\\ 1 & a_2 & a_2^2 & \\cdots & a_2^n \\\\ \\vdots & \\vdots& \\vdots & \\ddots & \\vdots \\\\ 1 & a_m & a_m^2 & \\cdots & a_m^n \\end{pmatrix} \\begin{pmatrix} 1 & a_1 & a_1^2 & \\cdots & a_1^n \\\\\\\\ 1 & a_2 & a_2^2 & \\cdots & a_2^n \\\\\\\\ \\vdots & \\vdots& \\vdots & \\ddots & \\vdots \\\\\\\\ 1 & a_m & a_m^2 & \\cdots & a_m^n \\end{pmatrix}manual123456789101112131415161718192021222324252627282930313233343536373839import numpy as npx_data = [1.0,2.0,3.0]y_data = [2.0,4.0,6.0]w1=1.0 # a random guessw2=1.0b=0.01# our model for the forward passdef forward(x): return x*x * w2 + x * w1 + b# loss fuctiondef loss(x,y): y_pred=forward(x) return(y_pred-y) * (y_pred-y)# compute gradientdef gradient(x,y,b): # d_loss/d_w - derivative of the loss wrt the weights grad = (2*x*((x*x*w2)+(x*w1)+b+y)) + (2*x*x*(x*x*w2+x*w1+b-y)) return grad# before trainingprint(&quot;predict before training&quot;,&quot;4 hours: &quot;, forward(4))for epoch in range(100): for x_val, y_val in zip(x_data,y_data): grad = gradient(x_val,y_val,b) w1 = w1 - 0.01*grad w2 = w2 - 0.01*grad# print(&quot;\\tgrad: &quot;, x_val, y_val, grad) l = loss(x_val,y_val)# print(&quot;progress:&quot;, epoch, &quot;w1=&quot;, w1, &quot;w2=&quot;, w2, &quot;b=&quot;, b, &quot;loss=&quot;,l)# after trainingprint(&quot;predict (after training)&quot;, &quot;4 hours: &quot;, forward(4)) predict before training 4 hours: 20.01 predict (after training) 4 hours: 5.659396343566748 using pytorch12345import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimtorch.manual_seed(1) &lt;torch._C.Generator at 0x19b4d450090&gt; 12345x_train = torch.FloatTensor([[1.0],[2.0],[3.0]])y_train = torch.FloatTensor([[2.0],[4.0],[6.0]])w1 = torch.zeros(1, requires_grad=True)w2 = torch.zeros(1, requires_grad=True)b = torch.zeros(1, requires_grad=True) 123456789101112131415161718192021# optimizer 설정optimizer = optim.SGD([w1, w2 ,b], lr=1e-4)nb_epochs = 50000for epoch in range(nb_epochs + 1): # H(x) 계산 hypothesis = x_train*x_train*w1+x_train*w2+b # cost 계산 cost = torch.mean((hypothesis - y_train) ** 2) # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() if epoch % 10000 == 0: print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} b: {:.3f} Cost: {:.6f}'.format( epoch, nb_epochs, w1.item(), w2.item(), b.item(), cost.item() )) Epoch 0/50000 w1: 0.005 w2: 0.002 b: 0.001 Cost: 18.666666 Epoch 10000/50000 w1: 0.393 w2: 0.686 b: 0.640 Cost: 0.102423 Epoch 20000/50000 w1: 0.304 w2: 0.854 b: 0.824 Cost: 0.026239 Epoch 30000/50000 w1: 0.270 w2: 0.924 b: 0.879 Cost: 0.016376 Epoch 40000/50000 w1: 0.256 w2: 0.958 b: 0.890 Cost: 0.014763 Epoch 50000/50000 w1: 0.248 w2: 0.980 b: 0.885 Cost: 0.014186 1print((4*4*w1.item()+4*w2.item()+b).item()) 8.777408599853516 Optimizer참조 Logistic RegressionBinary Classification using sigmoid12345%matplotlib inlineimport numpy as np # 넘파이 사용import matplotlib.pyplot as plt # 맷플롯립사용def sigmoid(x): # 시그모이드 함수 정의 return 1/(1+np.exp(-x)) 1234567891011x = np.arange(-5.0, 5.0, 0.1)y1 = sigmoid(0.5*x)y2 = sigmoid(x)y3 = sigmoid(2*x)plt.plot(x, y1, 'r', linestyle='--') # W의 값이 0.5일때plt.plot(x, y2, 'g') # W의 값이 1일때plt.plot(x, y3, 'b', linestyle='--') # W의 값이 2일때plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가plt.title('Sigmoid Function')plt.show() Activation Functions 12w = torch.tensor(2.0, requires_grad=True)z = 2*w*w + 5 1z.backward() 1print(w.grad) tensor(8.) 1","link":"/2021/01/25/Pytorch/Pytorch1/"}],"tags":[],"categories":[{"name":"Intro","slug":"Intro","link":"/categories/Intro/"},{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"Pytorch","slug":"AI/Pytorch","link":"/categories/AI/Pytorch/"}]}