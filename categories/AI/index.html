<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: AI - blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="blog"><meta name="msapplication-TileImage" content="./img/favicon3.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="blog"><meta property="og:url" content="https://keonwoochoi.github.io/"><meta property="og:site_name" content="blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://keonwoochoi.github.io/img/og_image.png"><meta property="article:author" content="Keonwoo Choi"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://keonwoochoi.github.io"},"headline":"blog","image":["https://keonwoochoi.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Keonwoo Choi"},"description":""}</script><link rel="icon" href="/./img/favicon3.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-10-tablet is-10-desktop is-10-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">AI</a></li></ul></nav></div></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/19/BoostCamp/Day39/"><img class="fill" src="/img/boostcamp.png" alt="Day39"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-19T06:59:34.000Z" title="2021-3-19 3:59:34 ├F10: PM┤">2021-03-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-19T07:03:02.459Z" title="2021-3-19 4:03:02 ├F10: PM┤">2021-03-19</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">11 minutes read (About 1658 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/19/BoostCamp/Day39/">Day39</a></h1><div class="content"><h2 id="fixed-point-amp-floating-point"><a href="#fixed-point-amp-floating-point" class="headerlink" title="fixed point &amp; floating point"></a>fixed point &amp; floating point</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111742784-0d227500-88cc-11eb-899c-4290722aeb8e.png" alt="image-20210319114019099"></li>
<li><p>fixed-point : 정수 부분과 소수 부분을 그대로 구역을 나누눠 저장, 32bit Floating point방법에 비하여 메모리를 비 효율적으로 사용하는 형태</p>
</li>
<li><p>모두 실수를 표현하는 방법이며, 같은 bit를 사용할 때 표현할 수 있는 수의 갯수는 서로 같다. 다만 floating point는 fixed point에 비해 정밀도가 떨어지고, 대신 더 넓은 범위의 값들을 표현할 수 있다.</p>
</li>
<li>floating point를 연산할 때는 FPU(Floating Point Unit)이 사용되는데, 회로의 크기도 크고 메모리도 많이 잡아먹는다. 따라서 정밀도를 포기하고 floating point 대신 int를 사용한다면 모델경량화에 도움이 될 것이다.</li>
</ul>
<h2 id="Quantization"><a href="#Quantization" class="headerlink" title="Quantization"></a>Quantization</h2><ul>
<li>아날로그 데이터, 즉 연속적인 값을 디지털 데이터, 즉 띄엄띄엄한 값으로 바꾸어 근사하는 과정</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742786-0d227500-88cc-11eb-8b4d-6903681de7b0.png" alt="image-20210319114829780"></li>
<li>FP number를 int8 등의 정수 보다 적은 bit의 자료형으로 mapping하여 정보를 잃는 대신 필요한 메모리를 줄이고 컴퓨팅 속도를 빠르게 하기 위한 compression 기법</li>
<li>float32는 크기에 비해 적은 범위를 사용하고 있고, 이 것을 int8에 mapping 시키면 모든 공간 사용이 가능하지만 정보 손실이 일난다</li>
<li>특징<ul>
<li>model size 감소</li>
<li>memory bandwith requirements 감소</li>
<li>inference 에서 속도를 높이는 기초적인 방법이다.</li>
</ul>
</li>
<li>연산 속도 가속화:이는 FPU(Floating Point Unit)가 하던 연산을 ALU(Arithmetic and Logic Unit)가 대체할 수 있게 되기 때문이다.</li>
</ul>
<h2 id="Affine-quantization"><a href="#Affine-quantization" class="headerlink" title="Affine quantization"></a>Affine quantization</h2><ul>
<li>Affine transform은 어떤 linear map에서 형태는 똑같지만(닮음은 유지하면서), 비율이나 크기만 작아지고 커지는 transform<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111742790-0e53a200-88cc-11eb-8372-42fce9c8f356.png" alt="image-20210319120555233"></li>
<li>linear + activation</li>
</ul>
</li>
<li>Affine quantization은 이전 수식이나 형태를 유지하면서, 모델의 비율이나 크기만 작고 크게 quantization을 수행하는 것</li>
<li>Float -&gt; INT, INT -&gt; Float와 같은 quantization을 수행하고, 중간중간 Float, int형으로 variance의 scale만 바뀌었을뿐, 모델의 형태 자체가 변하지는 않는다</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742794-0e53a200-88cc-11eb-8ec0-84b1586615fd.png" alt="image-20210319120645300"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742796-0eec3880-88cc-11eb-9c15-71542467c066.png" alt="image-20210319120718936"></li>
</ul>
<h2 id="Quantizing-activation-and-weights"><a href="#Quantizing-activation-and-weights" class="headerlink" title="Quantizing activation and weights"></a>Quantizing activation and weights</h2><ul>
<li>activation과 weight 모두 quantizing을 할 수 있다</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742797-0eec3880-88cc-11eb-90e2-93cebd340fb0.png" alt="image-20210319120755093"><ul>
<li>ReLU activation function을 3-bit로 quantizatin</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742799-0f84cf00-88cc-11eb-84b4-84ccccf0bb70.png" alt="image-20210319120821079"><ul>
<li>Activation function에 quantization을 수행할 수 있고, weight 자체에다 quantization을 할 수 있다</li>
</ul>
</li>
</ul>
<h2 id="Differentiating-quantized-values"><a href="#Differentiating-quantized-values" class="headerlink" title="Differentiating quantized values"></a>Differentiating quantized values</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111742801-0f84cf00-88cc-11eb-81e9-4dbcd5f01dcc.png" alt="image-20210319120857040"><ul>
<li>Quantization의 문제는 backpropagation을 할 때 미분이 안되는 문제점을 가지고 있다</li>
<li>Quantization을 하게 되면 계단 형태의 함수가 발생하게 되고, flat 한 부분은 미분을 할 수 없기 때문</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742802-101d6580-88cc-11eb-8f98-10cc18d52eb2.png" alt="image-20210319121542673"><ul>
<li>backpropagation을 할 때 $∂y\over∂x$를 Quantization 하기 전의 값으로 돌려놓고서 Loss를 계산합니다.</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742803-10b5fc00-88cc-11eb-88b2-7c8edad3d1bd.png" alt="image-20210319121757407"><ul>
<li>또는 smoothing 해서 계단형으로 사용하기도 한다.</li>
</ul>
</li>
</ul>
<h2 id="Quantization의-종류"><a href="#Quantization의-종류" class="headerlink" title="Quantization의 종류"></a>Quantization의 종류</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111742806-10b5fc00-88cc-11eb-9f16-726e979453a6.png" alt="image-20210319122045563"><ul>
<li>어떤 것을 양자화할 것인지(weight, activaiton), 어떻게 할 건지(Dynamic, Static), 얼마나 할건지(mixed-precise, 16bit, 8bit, …), 언제 할 건지(Post-training, Quantization-aware training) 등</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742808-114e9280-88cc-11eb-93cb-33abf7bbc2ae.png" alt="image-20210319122213238"></li>
<li><strong>Dynamic quantization(DQ)</strong>: training할 때에는 activation은 그대로 두고, weight만 quantization했다가 인퍼런스할 때에 activation도 quantization을 하는 방법, LSTM, Transformer<ul>
<li>동적 양자화 기법은 보통 모델의 inference time 대부분을 메모리 로딩 시간이 잡아먹는 경우 많이 활용</li>
</ul>
</li>
<li><strong>Static quantization(PTQ)</strong>: 정적 양자화는 Post Training Quantization 또는 PTQ라고 불린다 . inference 때weight과 activation을 모두 양자화하는 기법이다, CNN</li>
<li><strong>Quantization aware training(QAT)</strong>: trainig 과정 중 quatization 될 것을 학습하도록 만드는 방법<ul>
<li>fake node를 두어 추후 quantize 되었을 때의 영향을 미리 simulation</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742811-114e9280-88cc-11eb-827e-920f7733cf14.png" alt="image-20210319130111896"></li>
</ul>
</li>
</ul>
<h2 id="Knowledge-distillation"><a href="#Knowledge-distillation" class="headerlink" title="Knowledge distillation"></a>Knowledge distillation</h2><ul>
<li>Knowledge distillation은 teacher network와 student network의 output의 차이를 좁히는 방법으로 지식을 전수하는 효과를 내는 것이다. 크기가 작은 모델이 이를 통해 비슷한 성능을 낼 수 있게 되기 때문에 모델을 압축할 때 사용된다.</li>
<li>transfer learning: 다른 도메인에 적용하는 것</li>
<li>distillation: 동일한 지식을 작은 모델에 전달</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111742818-13b0ec80-88cc-11eb-9f44-87749a6baf01.png" alt="image-20210319150455367"></li>
<li><p>logit와 sigmoid는 서로 역함수 관계</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111742816-127fbf80-88cc-11eb-9e58-937bf646ff27.png" alt="image-20210319144446801"></p>
</li>
<li><p>위쪽에서는 soft prediction과 soft label에 대한 KL Divergence</p>
</li>
<li><p>아래 쪽에서는 hard prediction과 hard label에 대한 softmax를 적용</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111742828-14e21980-88cc-11eb-9756-5b5dc96d2f60.png" alt="image-20210319152524779"></p>
<ul>
<li>soft label은 hard prediction과는 다르게 확률값들로 표현, dog와 cat은 가까운 거리를 가지고 있고, dog와 car는 먼 거리를 가지고 있다</li>
<li>이 개념을 Teacher-Student model에 집어넣기 위해 Softmax에 Temperature (T) 값을 집어 넣게 됩니다.</li>
</ul>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111742813-11e72900-88cc-11eb-8e1d-b39d7a456ea1.png" alt="image-20210319130813268"></p>
</li>
<li><script type="math/tex; mode=display">L(x;w) = \alpha * KL(teacher, student) + (1-\alpha)*Cross-Entropy(y,student)</script></li>
<li><pre><code class="lang-python">    KD_loss = nn.KLDivLoss(reduction=&#39;batchmean&#39;)(F.log_softmax(outputs/T, dim=1),
                F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \
                F.cross_entropy(outputs, labels) * (1. - alpha)
</code></pre>
</li>
<li><blockquote>
<p>You can refer to the definition/document of PyTorch’s KL Divergence loss (KLDivLos). Here it requires inputs to be probability distributions and log-probability distributions, and that’s why we’re using softmax and log-softmax on teacher/student outputs</p>
</blockquote>
</li>
<li><code>nn.KLDivLoss</code> 클래스의 기대 input은 student에 대한 log_softmax와 teacher에 대한 softmax</li>
</ul>
<h2 id="Zero-mean-assumption"><a href="#Zero-mean-assumption" class="headerlink" title="Zero-mean assumption"></a>Zero-mean assumption</h2><ul>
<li>distillation은 model compression 측면에서 zero-mean을 만족해야 한다</li>
</ul>
<h2 id="Feautre-distillation"><a href="#Feautre-distillation" class="headerlink" title="Feautre distillation"></a>Feautre distillation</h2><ul>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111742820-13b0ec80-88cc-11eb-9124-aef98b7e588c.png" alt="image-20210319151409093"></p>
<ul>
<li>Teacher network에서 나온 soft label 결과를 가지고 training 하는 것이 아니라 중간중간의 layer마다 값을 비교하는 방법</li>
</ul>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111742824-14498300-88cc-11eb-81a3-9e225610127b.png" alt="image-20210319151513572"></p>
<ul>
<li>뽑아낸 Knowledge를 ReLU 앞에 놓아가지고 Distillation을 수행하는 방법</li>
</ul>
</li>
</ul>
<h2 id="Data-free-knowledge-distillation"><a href="#Data-free-knowledge-distillation" class="headerlink" title="Data-free knowledge distillation"></a><strong>Data-free knowledge distillation</strong></h2><ul>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111742826-14498300-88cc-11eb-818e-246ae1e98f83.png" alt="image-20210319152242974"></p>
<ul>
<li>teacher, student 모두 오리지널 데이터가 필요한 문제-&gt;데이터가 필요 없도록</li>
<li>data에 종속되지 않는 (data가 없거나 매우 적은 양)으로 knowledge distillation</li>
</ul>
</li>
<li><p>학습시킨 student model을 deploy할 때, Data center에서 뽑아온 metadata와 함께 deploy 하고, stduent model이 필요한 시점에 필요한 데이터를 reconstruction 한다는 것</p>
</li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/18/BoostCamp/Day38/"><img class="fill" src="/img/boostcamp.png" alt="Day38"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-18T06:38:26.000Z" title="2021-3-18 3:38:26 ├F10: PM┤">2021-03-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-18T06:41:52.164Z" title="2021-3-18 3:41:52 ├F10: PM┤">2021-03-18</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">9 minutes read (About 1339 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/18/BoostCamp/Day38/">Day38</a></h1><div class="content"><h1 id="Acceleration"><a href="#Acceleration" class="headerlink" title="Acceleration"></a>Acceleration</h1><ul>
<li>대기시간(Latency)은 요청한 데이터가 도달할 때까지 걸리는 시간이다.</li>
<li>대역폭(Bandwidth)은 단위시간 동안 전달할 수 있는 데이터의 최대치이다.</li>
<li>처리량(Throughput)은 단위시간 동안 실제로 전달되는 데이터의 양으로, 병렬 처리(Parallel processing)은 처리량을 늘리기 위한 것이다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111583668-1c3bf100-8800-11eb-88c2-79ab98f52be6.png" alt="image-20210318014304499"></li>
<li>GPU를 사용하는 이유, 병렬 처리가 가능한 라이브러리를 사용하는 이유, 모두 <strong>가속화</strong>와 밀접한 관련이 있다.</li>
</ul>
<h2 id="Hardware-acceleration"><a href="#Hardware-acceleration" class="headerlink" title="Hardware acceleration"></a>Hardware acceleration</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583669-1c3bf100-8800-11eb-8ec0-9504ab822a25.png" alt="image-20210318103611350"></li>
<li>SoC(System on Chip)는 CPU, GPU 등 다양한 시스템의 구성요소를 칩 하나에 집약한 것으로, 이를 사용하는 대표적인 제품에는 Apple의 M1이 있다.</li>
<li>FPGA(Field-programmable gate array)는 사용자가 원하는 용도에 따라 내부 회로를 코딩해서 바꿔 사용할 수 있는 칩이다.</li>
</ul>
<h3 id="공간-복잡도와-관련있는-압축은-소프트웨어에서-해결하고-시간-복잡도와-관련있는-가속은-하드웨어에서-해결"><a href="#공간-복잡도와-관련있는-압축은-소프트웨어에서-해결하고-시간-복잡도와-관련있는-가속은-하드웨어에서-해결" class="headerlink" title="공간 복잡도와 관련있는 압축은 소프트웨어에서 해결하고, 시간 복잡도와 관련있는 가속은 하드웨어에서 해결"></a>공간 복잡도와 관련있는 압축은 소프트웨어에서 해결하고, 시간 복잡도와 관련있는 가속은 하드웨어에서 해결</h3><h2 id="Deep-learning-compiler"><a href="#Deep-learning-compiler" class="headerlink" title="Deep learning compiler"></a>Deep learning compiler</h2><ul>
<li><p>딥러닝 모델을 특정 디바이스에서 효율적으로 동작시키기 위해서는 해당 디바이스에 최적화된 코드가 필요하다.  이러한 작업을 자동으로 지원해주는 도구가 바로 DL compiler</p>
</li>
<li><p>LLVM</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583670-1cd48780-8800-11eb-8858-22fc6e9a9c6b.png" alt="image-20210318104309130"></li>
<li>원래는  ‘언어의 종류 x 아키텍처의 종류’만큼 복수의 컴파일러가 필요하다</li>
<li>LLVM IR을 사용하면 Frontend에서 어떤 언어가 들어오던 원하는 architecture로 변환해</li>
</ul>
</li>
<li>MLIR<ul>
<li>MLIR은 LLVM의 Machine Learning 버전</li>
<li>MLIR에서는 중간에 있는 compiler들을 조립 후 통합해서 “통합 Framework”를 사용할 수 있게 만들었다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111583671-1cd48780-8800-11eb-9835-3c809b2ec075.png" alt="image-20210318104901155"></li>
</ul>
</li>
<li>DL complier에 적용하는 Hardware-specific optimizations<ul>
<li>Hardware Intrinsic Mapping</li>
<li>Memory Allocation &amp; Fetching</li>
<li>Memory Latency Hiding</li>
<li>Loop Oriented Optimization Techniques</li>
<li>Parallelization</li>
</ul>
</li>
</ul>
<h2 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h2><ul>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111583674-1d6d1e00-8800-11eb-91e7-2e02a8d736cd.png" alt="image-20210318105045016"></p>
</li>
<li><p>Pruning이란 중요하지 않고 반복되는 부분을 잘라내는 것을 뜻한다. 이를 통해 정보의 손실은 일어나겠지만, 모델의 복잡도를 줄여 일반화 성능을 높이고 속도를 높이는 효과를 낸다.</p>
</li>
<li>딥러닝에서는 Pruning을 통해 중요하지 않은 것(weight이 0에 가까운 것)을 줄인다.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583675-1d6d1e00-8800-11eb-8285-6890580eca16.png" alt="image-20210318105106354"></li>
</ul>
</li>
<li>Pruning과 Dropout은 비슷해보이지만, Pruning은 Dropout과 달리 버린 부분을 되살릴 수 없다. 또한 Dropout은 학습 시 사용하는 뉴런의 조합의 바꿔서 앙상블 효과를 낼 뿐 테스트를 할 때는 모든 뉴런을 전부 사용한다.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583676-1e05b480-8800-11eb-9380-6f0d8c4a2872.png" alt="image-20210318105138679"></li>
</ul>
</li>
<li>L1, L2 regularization<ul>
<li>모델 과적합을 방지하기 위한 방법 중 하나도 weight regularization이 있다.</li>
<li>loss function 에 weight L1 norm, L2 norm 항을 추가한 것을 각각 L1, L2 regulation 이라 칭한다.</li>
<li>pruning 비율에 따른 accuracy 감소 비율<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583678-1e05b480-8800-11eb-928e-4724bd9a32ff.png" alt="image-20210318113255066"></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111583679-1e9e4b00-8800-11eb-9a07-4a8542db5985.png" alt="image-20210318122725337"><ul>
<li>pruning 후 retrain 할 때 weight regularization 항을 추가하여 과적합(파란점)을 방지(빨간점)한다. </li>
</ul>
</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111583681-1e9e4b00-8800-11eb-8e13-6b78230e8c34.png" alt="image-20210318122805894"><ul>
<li>pruning 하면 parameter 수가 감소하고, accuracy도 낮아진다. 동일 accuracy를 보일 경우 속도는 느려진다.  (tradeoff)</li>
</ul>
</li>
</ul>
<h1 id="Pruning-category"><a href="#Pruning-category" class="headerlink" title="Pruning category"></a>Pruning category</h1><p><img src="https://user-images.githubusercontent.com/46857207/111583682-1f36e180-8800-11eb-916b-0e852787c2bd.png" alt="image-20210318123052587"></p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583684-1f36e180-8800-11eb-9919-57ad6d543d89.png" alt="image-20210318123146640"><ul>
<li>Unstructured Pruning : weight를 아무 규격 없이 잘라내는 pruning</li>
<li>Structured Pruning : weight를 channel / layer 단위로 규격을 잡아 제거하는 pruning, 규격을 잡아 제거했기 때문에 hardward optimization이 잘 된다.</li>
</ul>
</li>
</ul>
<h2 id="Iterative-pruning"><a href="#Iterative-pruning" class="headerlink" title="Iterative pruning"></a><strong>Iterative pruning</strong></h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583689-1fcf7800-8800-11eb-9164-b95e4ac5fcea.png" alt="image-20210318124855689"><ul>
<li>Pruning을 한 번에 많이 수행하게 되면, 많은 weight가 사라지게되어 pruning loss가 줄어든 후 Accuracy가 다시 올라가지 않아서 Iterative (반복적)으로 수행</li>
<li>pruning loss -&gt; retraining -&gt; pruning loss -&gt; retraining …</li>
</ul>
</li>
</ul>
<h2 id="Lottery-ticket-hypothesis"><a href="#Lottery-ticket-hypothesis" class="headerlink" title="Lottery ticket hypothesis"></a>Lottery ticket hypothesis</h2><ul>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111583691-20680e80-8800-11eb-8cb2-9a9cf4680133.png" alt="image-20210318125107838"></p>
</li>
<li><p>pruning하기 전 얻었던 Accuracy (91%)를 pruning하고 나서도 얻을 수 있는 subnetwork가 원래 network 안에 존재할 것이다라는 가설</p>
</li>
<li>original network의 initialization된 parameter를 그대로 사용해야 Accuracy가 잘 나오는 subnetwork.</li>
<li>한계점은 subnetwork를 찾기 위해서는 어짜피 original network를 train해서 찾아야 한다는 것</li>
<li>한계점을 극복하기 위해 차후에 나온 방법(적은 cost)</li>
<li>Iterative Magnitude pruning<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583693-20680e80-8800-11eb-90fd-9c269f4f9c2f.png" alt="image-20210318132437123"></li>
<li>weight의 크기 기준으로 정렬해서 크기가 낮은 기준으로 잘라낸다는 의미</li>
<li>prune-&gt;mask를 씌우고 구조만 가지고 와서 다시 init</li>
</ul>
</li>
<li>Iterative Magnitude Pruning with Rewinding<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583694-2100a500-8800-11eb-8dd8-1b5ae6a5ca7f.png" alt="image-20210318132705337"></li>
<li>k번까지만 train 시킨 후, 이때의 네트워크를 저장</li>
<li>이후 수렴할 때 까지 훈련하고 pruning한 후 마스크를 적용해준다.</li>
<li>Retrain 시, 마스크가 적용된 네트워크를 k번 학습하고 저장해놓은 네트워크를 이용하여 초기화해준다. </li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/18/BoostCamp/Day37/"><img class="fill" src="/img/boostcamp.png" alt="Day37"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-18T06:38:19.000Z" title="2021-3-18 3:38:19 ├F10: PM┤">2021-03-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-21T10:39:21.291Z" title="2021-3-21 7:39:21 ├F10: PM┤">2021-03-21</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">8 minutes read (About 1148 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/18/BoostCamp/Day37/">Day37</a></h1><div class="content"><h2 id="Space-Time-Trade-off"><a href="#Space-Time-Trade-off" class="headerlink" title="Space-Time Trade-off"></a>Space-Time Trade-off</h2><ul>
<li>공간(space)과 시간(time)은 서로 trade-off 관계에 있음.<ul>
<li>압축되지 않은(uncompressed) 데이터를 저장한다면 더 많은 공간이 필요한 대신, 더 적은 시간이 걸림.</li>
<li>데이터를 압축(compressed)해 저장한다면 더 적은 공간이 필요한 대신, 더 많은 시간이 걸림.</li>
</ul>
</li>
<li>공간(space)은 problem space(state space)와 search space(solution space)로 이루어져 있음.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901700-e8b9ca80-8a7c-11eb-94c3-758edb66f576.png" alt="image-20210317123006768"><ul>
<li>problem space는 문제를 해결하는 과정에 존재하는 모든 요소들의 공간이며, search space는 문제의 조건을 만족하는 요소들의 공간</li>
<li>search space: 모든 노드가 연결되어 있고, cycle이 존재하지 않은 그래프. 정답 후보.</li>
<li>optimal solution: 그 중 1개(또는 일부)의 cost가 가장 작은 optimal solution. 정답.</li>
</ul>
</li>
<li>시간과 공간의 제약 하에 주어진 문제(problem space)를 풀기 위해 제약 조건을 만족하는 집합(search space; feasible space) 중 가장 높은 performance를 내는 것을 찾음.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901701-e9526100-8a7c-11eb-808c-85153d3bda54.png" alt="image-20210317123048415"></li>
</ul>
</li>
</ul>
<h2 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h2><ul>
<li>불확실성의 측정</li>
<li>정렬된 왼쪽 상태를 Low Entropy, 불규칙한 오른쪽 상태<strong>를 </strong>High Entropy</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901702-e9eaf780-8a7c-11eb-8346-27c5880e5f32.png" alt="image-20210317124127726"><ul>
<li>문제해결도 엔트로피(entropy) 관점에서 볼 수 있음.</li>
<li>문제를 푼다는 특정한 initial state에서 무질서한 problem solving 과정을 통해 엔트로피가 낮은 최적의 terminal state에 도달하는 것.</li>
</ul>
</li>
</ul>
<h2 id="Parameter-search-amp-Hyper-parameter-search"><a href="#Parameter-search-amp-Hyper-parameter-search" class="headerlink" title="Parameter search &amp; Hyper-parameter search"></a>Parameter search &amp; Hyper-parameter search</h2><h3 id="Parameter-Search"><a href="#Parameter-Search" class="headerlink" title="Parameter Search"></a>Parameter Search</h3><ul>
<li>모델이 학습하며 가중치(weight)를 찾아감.<ul>
<li>class를 가장 잘 나누는 최적의 결정경계(decision boundary) 탐색.</li>
<li>loss가 커지거나 작아지는 일종의 binary search 문제. 단, global minima 보장하지 않음.</li>
</ul>
</li>
</ul>
<h3 id="Hyperparameter-Search"><a href="#Hyperparameter-Search" class="headerlink" title="Hyperparameter Search"></a>Hyperparameter Search</h3><ul>
<li><p>사람이 찾아주는 값.</p>
</li>
<li><p>한번의 hyperparameter 탐색에 cost가 너무 큼.</p>
</li>
<li><p>적은 cost를 보장하는 hyperparameter를 탐색 최적화 알고리즘 필요.</p>
</li>
<li><p><strong>Grid Search</strong></p>
<ul>
<li>hyperparameter들의 후보를 골라 조합 탐색.</li>
<li>선택한 grid의 중간 단계는 탐색하지 않음.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901703-e9eaf780-8a7c-11eb-95a2-5f3f9f2eb20d.png" alt="image-20210317124753800"></li>
</ul>
</li>
<li><p><strong>Random Search</strong></p>
<ul>
<li>범위 내의 랜덤한 값을 탐색.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901705-ea838e00-8a7c-11eb-85a7-cf9d8e218fa9.png" alt="image-20210317124812006"></li>
</ul>
<p><strong>Bayesian Optimization</strong></p>
<ul>
<li>Surrogate model(대리모델)을 만들어 학습해가며 최적의 hyperparameter 조합 탐색.<ul>
<li>Surrogate Model은 ML 모델을 정의하는 hyperparameter들을 위한 머신러닝 모델</li>
</ul>
</li>
<li>반복연산으로 cost가 크고 모델의 hyperparameter도 찾아야 함.</li>
<li>Surrogate의 대표적인 process가 Gaussian process<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901706-ea838e00-8a7c-11eb-8e89-e04f32d02ec9.png" alt="image-20210317124842798"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901708-eb1c2480-8a7c-11eb-9208-0d39287d002f.png" alt="image-20210317124906390"><ul>
<li>파란색 음영 ⇒ covariance</li>
<li>초록색 함수 ⇒ acquisition function</li>
<li>검정색 함숫값도 크고, 파란색 음영도 작은 부분을 찾겠다는 것.</li>
<li>exploitation은 오른쪽 점 근처에 최적값이 존재할 것”이라고 예측</li>
<li>exploration은 표준편차가 가장 큰 점, 즉, 불확실성이 가장 높은 점 근처에 최적값이 존재하는 것이라고 예측</li>
<li>Mean과 Variance를 둘 다 고려해서 Acquisㅑtion max를 찾는다</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Neural-Architecture-Search-NAS"><a href="#Neural-Architecture-Search-NAS" class="headerlink" title="Neural Architecture Search(NAS)"></a><strong>Neural Architecture Search(NAS)</strong></h2><ul>
<li>여러 모델들의 Architecture들을 알고리즘, 딥러닝 등의 모델에 넣어서 가장 좋은 성능의 Architecture를 찾아내는 방법</li>
<li><strong>Search strategy</strong><ul>
<li>어디와 어디를 residual connection,fuse,depth-wise-seperate, 몇개의 layer..</li>
<li>grid search</li>
</ul>
</li>
<li>MnasNet</li>
<li>PROXYLESSNAS</li>
<li>ONCE-FOR-ALL</li>
</ul>
<h2 id="압축-Compression-amp-압축률-Compression-rate"><a href="#압축-Compression-amp-압축률-Compression-rate" class="headerlink" title="압축 (Compression) &amp; 압축률 (Compression rate)"></a>압축 (Compression) &amp; 압축률 (Compression rate)</h2><ul>
<li><p>압축은 손실 압축과 비손실 압축으로 나뉘는데, 손실 압축은 압축된 데이터를 복원할 때 손실이 일어나지만 더 높은 압축률을 가진다.</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901711-ebb4bb00-8a7c-11eb-9bdf-4faf5aeb81e2.png" alt="image-20210317125415127"></li>
</ul>
</li>
<li><p>Entropy 관점의 Encoding</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901713-ec4d5180-8a7c-11eb-8f4a-b17da321ad04.png" alt="image-20210317151456034"></li>
<li>Cross-entropy 부분은 Q(i)라는 codebook의 입장에서 P(i)라는 message(정답 set)를 encoding 했을 때의 나오는 무질서도를 의미</li>
<li>Entropy 부분은 위에서 주황색 식 H(p)H(p), 즉, P(i)라는 message를 P(i)라는 codebook으로 encoding 했을 때를 무질서도를 의미</li>
<li>이때 entropy의 값은 0이 나오지는 않고, P(i) 분포가 기본적으로 가지고 있는 minimum entropy가 나옴</li>
<li>Cross-entropy에서 Entropy를 뺸다는 의미</li>
</ul>
</li>
<li><p>압축 후의 모델 사이즈와 정확도 손실 측면에서 봤을 때, Pruning과 Quantization을 동시에 해주는 것이 좋은 결과를 낸다.</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901712-ec4d5180-8a7c-11eb-99dd-3eac78749056.png" alt="image-20210317125509350"></li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/16/BoostCamp/Day36/"><img class="fill" src="/img/boostcamp.png" alt="Day36"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-16T06:49:53.000Z" title="2021-3-16 3:49:53 ├F10: PM┤">2021-03-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-16T06:51:56.814Z" title="2021-3-16 3:51:56 ├F10: PM┤">2021-03-16</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">7 minutes read (About 1085 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/16/BoostCamp/Day36/">Day36</a></h1><div class="content"><h2 id="연역적-deductive-결정"><a href="#연역적-deductive-결정" class="headerlink" title="연역적 (deductive) 결정"></a>연역적 (deductive) 결정</h2><ul>
<li>가설 증명을 통해 현상을 추론</li>
<li>“전제가 참이면, 결론도 참”이라는 논리로 결정</li>
<li>전제에 따라 결과가 바뀜</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/46857207/111267316-1ebb1100-866f-11eb-8087-29b8730e952f.png" alt="image-20210315091729505"></p>
<h2 id="귀납적-inductive-결정"><a href="#귀납적-inductive-결정" class="headerlink" title="귀납적 (inductive) 결정"></a>귀납적 (inductive) 결정</h2><ul>
<li>가설을 경험적 자료와 비교해 추론</li>
<li>높은 확률로 참, 낮은 확률로 거짓</li>
<li>머신러닝(결정기)도 이에 해당</li>
</ul>
<h2 id="결정기"><a href="#결정기" class="headerlink" title="결정기"></a>결정기</h2><ul>
<li>결정기는 어떤 데이터를 가지고 최종 판단을 내리는 것을 말한다.</li>
<li>추천시스템과 같은 가벼운 의사결정부터 암 진단과 같은 무거운 결정까지 다양</li>
<li>딥러닝이 발전하기 전에는 모델의 성능이 좋지 않았기 때문에 가벼운 결정만 가능하였지만 발전 이후 정확도가 거의 100%에 가까워지면서 무거운 의사결정가지 가능</li>
</ul>
<h2 id="가벼운-결정기"><a href="#가벼운-결정기" class="headerlink" title="가벼운 결정기"></a>가벼운 결정기</h2><p>머신러닝(결정기)은 인간의 결정을 대신 해줌</p>
<p>서비스하기 위해 큰 데이터로 학습한 모델을 경량화해서 edge device에 탑재.</p>
<ul>
<li><code>경량화</code>: 필요한 것만 갖고 불필요한 것을 제거하여 규모를 줄이거나 가볍게 만드는 것</li>
<li><code>소형화</code>: 필요/불필요한 것을 구분하지 않고 규모를 줄이거나 가볍게 만드는 것</li>
</ul>
<h2 id="무엇을-경량화"><a href="#무엇을-경량화" class="headerlink" title="무엇을 경량화?"></a>무엇을 경량화?</h2><ul>
<li>모델이 가진 무의미한 정보량을 줄여서 경량화를 달성하는 것이 목표</li>
<li>무의미한 정보가 줄어들면, 정보의 밀도가 올라간다</li>
<li>모델 압축(Compression)과도 같다</li>
</ul>
<h2 id="어떻게-경량화"><a href="#어떻게-경량화" class="headerlink" title="어떻게 경량화?"></a>어떻게 경량화?</h2><ul>
<li>Pruning</li>
<li>Quantization</li>
<li>Knowledge Distillation</li>
<li>Filter Decomposition</li>
</ul>
<h2 id="모델-경량화-과정"><a href="#모델-경량화-과정" class="headerlink" title="모델 경량화 과정"></a>모델 경량화 과정</h2><p><img src="https://user-images.githubusercontent.com/46857207/111267389-32667780-866f-11eb-990a-0ef1fa9fcd83.png" alt="image-20210316125544301"></p>
<h2 id="Edge-device"><a href="#Edge-device" class="headerlink" title="Edge device"></a>Edge device</h2><p> cloud service나 on-premise(자체 서버)를 했을 때에 엄청난 비용이 발생</p>
<p>사생활 문제와 항상 네트워크에 연결되어 있어야 한다는 문제</p>
<p><img src="https://user-images.githubusercontent.com/46857207/111267326-211d6b00-866f-11eb-8ced-1b64c5dc8f40.png" alt="image-20210315094052051"></p>
<ul>
<li>Low cost</li>
<li>No privacy concerns</li>
<li>Stand-alone</li>
</ul>
<h2 id="Cloud-intelligence-edge-intelligence"><a href="#Cloud-intelligence-edge-intelligence" class="headerlink" title="Cloud intelligence, edge intelligence"></a>Cloud intelligence, edge intelligence</h2><p><img src="https://user-images.githubusercontent.com/46857207/111267332-224e9800-866f-11eb-8373-d76098b56d67.png" alt="image-20210315094300693"></p>
<p><img src="https://user-images.githubusercontent.com/46857207/111267335-22e72e80-866f-11eb-8771-0d8105241f9e.png" alt="image-20210315094420318"></p>
<ul>
<li>Edge intellignece는 Centralized intelligence와 비교</li>
<li>좌측은 중앙 서버의 과중이 심한 반면 우측의 Edge intelligence의 경우 비교적 중앙 서버가 과중이 심하지 않음</li>
<li>Edge intelligence에는 Edge Training, Edge Offloading, Edge Caching, Edge Inference 등이 있다.</li>
<li>우리가 많이 다루게 될 내용은 Edge Inference </li>
<li>Edge Inference를 적용하기 위해서는 Pytorch로 모델을 만든 뒤 Edge device에서 사용할 수 있다 </li>
</ul>
<p><img src="https://user-images.githubusercontent.com/46857207/111267392-32667780-866f-11eb-8780-583471cd7a86.png" alt="image-20210316130537701"></p>
<ul>
<li>Edge Inferenced의 depth </li>
<li>High level에서 사용하는 PyTorch나 Tensorflow 등으로 Model을 대부분 만들고  이것들이 Edge devices로 내려가기 위해서는 Low level IR로 <strong>Graph lowering</strong></li>
</ul>
<h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><p><strong>Computer Optimization</strong></p>
<ul>
<li><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbKwed4%2FbtqZ3eTdRT0%2FmcnaZL0KdrPGjdQuryVKL0%2Fimg.gif" alt="Computer Optimization"></li>
<li>컴퓨터가 문제를 푸는 과정</li>
<li>모든 combination을 고려하기 전까지 최적해를 알 수 없음. 반대로, 유한한 모든 combination을 고려하면 무조건 최적해 보장</li>
</ul>
<p><strong>ML Optimazation</strong></p>
<ul>
<li><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FoFGNM%2FbtqZ1I1Cuyt%2FikKgKui9N5mrT1dZAjdZuK%2Fimg.gif" alt="ML"></li>
<li>머신러닝이 문제를 푸는 과정</li>
<li>모든 상태를 고려하지 않고 현재 상태보다 낫게 업데이트해 최적화</li>
</ul>
<h1 id="Decision-problem-vs-optimization-problem"><a href="#Decision-problem-vs-optimization-problem" class="headerlink" title="Decision problem vs optimization problem"></a>Decision problem vs optimization problem</h1><h2 id="Decision-problem"><a href="#Decision-problem" class="headerlink" title="Decision problem"></a>Decision problem</h2><ul>
<li>조건을 만족한다면 문제를 푼 것. 하나의 결정.</li>
<li>무한한 자원으로 최고 성능을 내는 것.</li>
<li>Decision Spanning Tree (DST)<ul>
<li>upper bound가 정해져 있고, 그 upper bound만 만족하면 풀리는 문제</li>
</ul>
</li>
</ul>
<h2 id="Optimazation-problem"><a href="#Optimazation-problem" class="headerlink" title="Optimazation problem"></a>Optimazation problem</h2><ul>
<li>더이상 못찾을 때까지 decision을 연쇄적으로 반복.</li>
<li>제약 조건을 만족하면서 최고의 성능을 내는 것<ul>
<li>Minimum Spanning Tree(MST)</li>
<li>Dicision problem을 연쇄적으로 반복했을 때 해결</li>
</ul>
</li>
</ul>
<h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints"></a><strong>Constraints</strong></h2><p>무엇을 원하느냐에 따라 굉장히 달라짐(시간,돈…)</p>
<p><img src="https://user-images.githubusercontent.com/46857207/111267395-32ff0e00-866f-11eb-95ec-6155326423ff.png" alt="image-20210316133450139"></p>
<ul>
<li>Decision problem에서 무한한 자원 (infinite amount of resoources)을 사용한다고 가정</li>
<li>Optimization problem에서는 각각의 Decision problem에서 지불했던 Cost를 더해서 Constraints를 계산합니다. 이때, 모든 Cost를 더했을 때 Constraint를 넘으면 안된다</li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/11/BoostCamp/Day33/"><img class="fill" src="/img/boostcamp.png" alt="Day33"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-11T07:10:49.000Z" title="2021-3-11 4:10:49 ├F10: PM┤">2021-03-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-11T07:21:20.429Z" title="2021-3-11 4:21:20 ├F10: PM┤">2021-03-11</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">24 minutes read (About 3569 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/11/BoostCamp/Day33/">Day33</a></h1><div class="content"><h1 id="Object-detection"><a href="#Object-detection" class="headerlink" title="Object detection"></a>Object detection</h1><h2 id="What-is-object-detection"><a href="#What-is-object-detection" class="headerlink" title="What is object detection"></a>What is object detection</h2><ul>
<li>semantic segmentation보다 더 구체적</li>
<li>객체가 달라도 구분이 됨</li>
<li>Instance segmentation ⊂ Panoptic sementation</li>
<li>object detection=Classification + Box localization<ul>
<li>two-stage: box localization → classfication</li>
<li>one-stage: box + classfication 한번에</li>
</ul>
</li>
</ul>
<h2 id="What-are-the-applications"><a href="#What-are-the-applications" class="headerlink" title="What are the applications"></a>What are the applications</h2><ul>
<li>autonomous driveing</li>
<li>ocr</li>
</ul>
<h1 id="Two-stage-detector"><a href="#Two-stage-detector" class="headerlink" title="Two-stage detector"></a>Two-stage detector</h1><h2 id="Traditional"><a href="#Traditional" class="headerlink" title="Traditional"></a>Traditional</h2><ul>
<li>Gradient-based detector<ul>
<li>과거에는 경계선을 특징으로 모델링 , gradient 방향성을 활용한 선형 모델 사용</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749394-78dd6000-8284-11eb-8939-33f4b781b27e.png" alt="image-20210310091730224"></li>
</ul>
</li>
<li>selective search<ul>
<li>사람이나 특정 물체 뿐만 아니라, 다양한 물체 후보군의 영역 후보군을 지정해주는 방식</li>
<li>bounding box 제안-&gt; Proposal Algorithm</li>
<li>Over-segmentation: 영상을 색끼리 잘게 분할</li>
<li>merginig: 비슷한 feature를 가지는 영역들을 합침</li>
<li>반복해서 합쳐주다보면 object를 소수로 특정지음</li>
<li>특정지은 소수의 object 위치를 바운딩 박스로 나타냄</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749396-7975f680-8284-11eb-9a67-3e54ab12e29b.png" alt="image-20210310092048339"></li>
</ul>
</li>
</ul>
<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><ul>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110749518-927ea780-8284-11eb-8b6b-19c497d8f97d.png" alt="image-20210311104814315"></p>
</li>
<li><p>Selective Search를 통해 Region Proposal을 진행 → 2K 이하로 설정</p>
</li>
<li>각 Region Proposal을 CNN의 input size로 Warping</li>
<li>기존에 pre-trained된 CNN에 input으로 넣어서 Classification을 진행</li>
<li>SVM의 linear classifier만을 이용해서 클래스를 학습(fine-tuning)</li>
<li>단점<ul>
<li>모든 region proposal이 CNN에 입력값으로 들어가기 때문에 느림</li>
<li>Hand Designed된 selective search-&gt; 학습을 통한 성능향상에 한계</li>
</ul>
</li>
</ul>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749399-7a0e8d00-8284-11eb-93bc-ea409f518ab2.png" alt="image-20210310092641179"></li>
<li>영상 전체에 대한 Feature을 한번에 추출하고 이를 재활용해서 object detection</li>
</ul>
<ol>
<li>Conv layer를 통해 feature map 검출<ul>
<li>conv layer를 거쳤으므로 tensor형태가 된다</li>
<li>Fully Convolutional Network는 입력사이즈와 상관없이 Feature Map을 뽑아낼 수 있기 때문에 warp 필요 없다</li>
</ul>
</li>
<li>RoI Pooling: Feature를 여러번 재활용, Region Proposal이 제시한 물체의 후보 위치들(Bounding Box)에 대해서 RoI에 해당하는 Feature만 추출한다.<ul>
<li>RoI feature를 고정된 사이즈로 resampling한다</li>
</ul>
</li>
<li>classification : softmax, bbx regression로 위치 조정</li>
</ol>
<ul>
<li>selective search를 사용했으므로 성능 향상 한계</li>
</ul>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><ul>
<li>IoU<ul>
<li>두 영역의 OVERLAP 측정</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749401-7a0e8d00-8284-11eb-8126-cf48f190ea43.png" alt="image-20210310092844050"></li>
</ul>
</li>
<li>anchor box 각 위치에서 발생할 것 같은 박스를 미리 정의해둔 후보군<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749404-7aa72380-8284-11eb-9011-51add2eb2350.png" alt="image-20210310092945166"></li>
<li>box의 개수와 종류는 hyperparameter</li>
<li>Faster R-CNN에서는 9개 (scale 3 * 비율3)</li>
</ul>
</li>
<li>selective search 대신 RPN 모듈 제안<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749406-7aa72380-8284-11eb-8f51-62d5eae5554d.png" alt="image-20210310093216117"></li>
<li>Fast R-CNN과 마찬가지로 각각의 영상에서 공유되는 Feature Map을 미리 뽑아두고, 해당 Feature Map을 바탕으로 RPN에서 Region Proposal을 여러 개 제공하고 RoI Pooling을 실시한 다음 Classifier를 통해 정답을 예측</li>
</ul>
</li>
<li>Region Proposal Network<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749407-7b3fba00-8284-11eb-81d1-43144ac30ad2.png" alt="image-20210310093418147"></li>
<li>Conv Layer를 통해 나온 Feature Map에 Sliding Window 방식으로 돌면서 매 위치마다 미리 정의해둔 K개의 Anchor Box를 고려<ol>
<li>각 위치에서 256-d의 feature vector를 하나 추출</li>
<li>각 vector로 부터 Object인지 아닌지에 대한 score인 2K개 classification score (object, non-object)</li>
<li>k개의 Bounding Box위치를 regression하는 4k 개의 Coordinates (x,y,w,h 4개)<ul>
<li>Anchor Box를 촘촘하게 만들면 계산 속도가 느려지므로 대표적인 비율과 Scale만 정해두고 정교한 위치는 Regression 문제로 분할 정복</li>
</ul>
</li>
<li>각각의 Loss → Cross Entropy Loss + Regression Loss를 사용<ul>
<li>이 2가지 Loss가 RPN을 위한 요소가 되며, 전체 Target Task를 위한 RoI별 Classification Loss는 따로 하나가 추가가 되서 전체적으로 End-to-End로 학습을 진행한다.</li>
</ul>
</li>
</ol>
</li>
<li>RPN에서 object를 완전히 class로 분류한게 아니라, object인지 아닌지만 판단</li>
<li>RPN에서 classifier, regressor 두개의 모델이 필요하고, 마지막으로 classifier가 필요하다. 또 맨 처음 feature map을 뽑아낼 CNN도 필요하다. 총 4개의 모델이 각각이 아니라 한번에 학습된다.</li>
</ul>
</li>
<li>non maximum suppression<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749409-7bd85080-8284-11eb-9a84-cfd804c87e2e.png" alt="image-20210310093530590"></li>
<li>IoU스코어가 낮은 박스들은 제거하는 방식의 NMS 알고리즘을 추가한다.</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749411-7c70e700-8284-11eb-922c-d2d10de4c07a.png" alt="image-20210310093600826"></li>
</ul>
<h1 id="Single-stage-detector"><a href="#Single-stage-detector" class="headerlink" title="Single-stage-detector"></a>Single-stage-detector</h1><h2 id="Comparison-with-two-stage-detector"><a href="#Comparison-with-two-stage-detector" class="headerlink" title="Comparison with two-stage detector"></a>Comparison with two-stage detector</h2><ul>
<li>성능을 조금 포기하고 계산 속도를 확보해서 real-time detection 이 가능하도록한 모델</li>
<li>ROI pooling을 하지 않기 때문에 구조가 매우 간단한 편</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749412-7c70e700-8284-11eb-8c45-473994cb8d33.png" alt="image-20210310093821561"></li>
</ul>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749413-7d097d80-8284-11eb-9609-7988796876ae.png" alt="image-20210310093908226"></li>
<li>input image를 SxS grid로 나눈다</li>
<li>각 grid마다 바운딩박스 정보 4개 + object여부 1개 + Class 분류 확률 C개 = 5+C개의 정보를 한꺼번에 예측한다.</li>
<li>Faster R-CNN의 RPN과 마찬가지로, ground truth와의 IoU스코어가 높은 Anchor box를 positive로 간주하여 loss 계산</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749416-7d097d80-8284-11eb-94be-293724797c46.png" alt="image-20210310094058186"><ul>
<li>30 channel= 앵커박스를 grid마다 2개만 사용했고, class 개수는 총 20개여서 5*2+20=30</li>
<li>s는 convolution layer에서의 마지막 해상도로 결정됨</li>
<li>마지막 layer에서만 prediction을 수행하기 떄문에 localization 정확도 떨어짐</li>
</ul>
</li>
</ul>
<h2 id="single-shot-multibox-detector-SSD"><a href="#single-shot-multibox-detector-SSD" class="headerlink" title="single shot multibox detector(SSD)"></a>single shot multibox detector(SSD)</h2><ul>
<li>SSD는 Multi Scale object를 더 잘 처리하기 위해서 중간 Feature를 각 해상도에 적절한 Bounding Box 들을 출력할 수 있도록하는 Multi Scale구조를 만들었다.</li>
<li>각기 다른 크기의 feature map에서 정해진 크기의 anchor box를 사용하면, feature map 사이즈에 따라 anchor box가 scaling된다.</li>
<li>feature map 사이즈마다, anchor box를 적용하여 object를 탐지한다. 때문에 YOLO에 비해 계산량이 늘어난다</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749418-7da21400-8284-11eb-9014-aac4a48e9928.png" alt="image-20210310094304705"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749420-7e3aaa80-8284-11eb-9413-98015b4b5f28.png" alt="image-20210310094408490"><ul>
<li>SSD는 각 층마다 classifier적용</li>
<li>각 layer마다 (class수+위치정보4)에 anchor box개수를 곱해준다</li>
</ul>
</li>
</ul>
<h1 id="Two-stage-vs-One-stage"><a href="#Two-stage-vs-One-stage" class="headerlink" title="Two-stage vs One-stage"></a>Two-stage vs One-stage</h1><h2 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h2><ul>
<li>일반적으로object보다 background가 더 넗음-&gt; class imbalance</li>
<li>모든 구역에 대해 object detection하는 경우 negative를 결과로 내는 anchor boxrk 많아진다.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749421-7ed34100-8284-11eb-8e38-af8862cfade9.png" alt="image-20210310094634765"></li>
</ul>
</li>
<li>focal loss= cross entropy의 확장<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749423-7ed34100-8284-11eb-9e64-9c90ba6c20cb.png" alt="image-20210310094659841"></li>
<li>파란색 그래프가 기존의 Cross Entropy</li>
<li>나머지가 FL함수에서 $\gamma$값을 바꿔주었을 때의 그래프다.</li>
<li>학률term을 통해 잘 맞춘 애들은 더 낮은 loss를 주고 맞추지 못한 애들은 더 높은 loss를 준다.</li>
<li>FL함수는 정답률이 높을수록 gradient를 0에 가깝게 만들고, 정답률이 낮을수록 gradient가 가팔라지게</li>
<li>Loss 값 자체가 중요한게 아니라 gradient</li>
<li>$\gamma$값을 크게줄수록 적용 폭이 강해진다.</li>
<li>Q) gamma가 높을수록 loss가 낮아지지만 잘 찾지 못할 때 loss는 소폭 줄어들게 하고 잘 찾은 경우에서는 loss를 대폭 줄어들게 합니다</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749520-93173e00-8284-11eb-968e-c751c79106fd.png" alt="image-20210311133620851"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749522-93173e00-8284-11eb-9964-c4b4683ea665.png" alt="image-20210311133647011"></li>
</ul>
</li>
</ul>
<h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h2><ul>
<li>FPN<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749425-7f6bd780-8284-11eb-924f-92c5964eb125.png" alt="image-20210310094941624"></li>
<li>U-Net과 비슷한 구조다.</li>
<li>residual을 활용하여 더해줌으로써, low-level의 특징과 high-level의 특징을 둘 다 활용하면서 각 scale별로 object를 잘 찾기 위한 multi-scale 구조를 갖게된다. (concat이 아닌 더하기)</li>
<li>class 분류와 box 회귀 예측을 진행한다.</li>
</ul>
</li>
</ul>
<h1 id="Detection-with-Transform"><a href="#Detection-with-Transform" class="headerlink" title="Detection with Transform"></a>Detection with Transform</h1><ul>
<li>DETR</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749426-7f6bd780-8284-11eb-9011-080928167227.png" alt="image-20210310095209241"></li>
<li>CNN으로 Feature추출 후, Positional encoding을 더해준다.</li>
<li>encoder 학습 후, decoder에 query를 입력으로 준다. (query는 object 정보이거나, positional encoding 정보) 추가로, 최대 몇개의 box를 찾아줄지에 대한 값도 함께 준다. (N)</li>
<li>해당 query에 맞는 object를 찾아 box를 찾아준다. positional encoding의 경우 해당 위치 근처의 바운딩 박스와 class를 찾아준다</li>
</ul>
<h1 id="Visualizing-CNN"><a href="#Visualizing-CNN" class="headerlink" title="Visualizing CNN"></a>Visualizing CNN</h1><h2 id="What-is-CNN-visualization"><a href="#What-is-CNN-visualization" class="headerlink" title="What is CNN visualization"></a>What is CNN visualization</h2><ul>
<li>딥러닝 네트워크= black box</li>
<li>CNN의 경우 시각화가 가능하다.</li>
<li>visualization as debugging tool</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749427-80046e00-8284-11eb-845a-face0dfb2896.png" alt="image-20210310101554404"><ul>
<li>low-level feature는 방향성이 있는 선, 동그란 블록</li>
<li>high-level로 갈수록 의미있는 feature</li>
</ul>
</li>
</ul>
<h2 id="Vanilla-example-filter-visualization"><a href="#Vanilla-example-filter-visualization" class="headerlink" title="Vanilla example: filter visualization"></a>Vanilla example: filter visualization</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749428-809d0480-8284-11eb-9807-59b73cd6e9fb.png" alt="image-20210310101738368"><ul>
<li>AlexNet 첫번째 층의 필터를 시각화</li>
<li>해당 필터를 이미지에 적용했을 때의 결과(하나의 필터만 적용했기 때문에 밝기값만 가지는 1채널 이미지)</li>
<li>높은 층으로 갈수록 필터도 채널 수가 늘어나 고차원의 필터를 시각화하기도 어렵고, 시각화 하더라도 사람이 해석할 수 없다</li>
</ul>
</li>
</ul>
<h2 id="How-to-visualize-neural-network"><a href="#How-to-visualize-neural-network" class="headerlink" title="How to visualize neural network"></a>How to visualize neural network</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749431-809d0480-8284-11eb-8320-f9de1cd8fdf5.png" alt="image-20210310102033215"><ul>
<li>왼쪽으로 갈수록 모델 자체를 분석</li>
<li>른쪽으로 갈수록 모델의 출력 결과가 왜 그렇게 나타났는지 분석</li>
</ul>
</li>
</ul>
<h1 id="Analysis-of-model-behaviors"><a href="#Analysis-of-model-behaviors" class="headerlink" title="Analysis of model behaviors"></a>Analysis of model behaviors</h1><h2 id="Embedding-feature-analysis"><a href="#Embedding-feature-analysis" class="headerlink" title="Embedding feature analysis"></a>Embedding feature analysis</h2><p>High-level layer에서 얻는 High-level feature들을 분석하는 방법</p>
<ul>
<li><p>nearest neighbors in a feature space</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110749433-81359b00-8284-11eb-82bd-232d7dd73a71.png" alt="image-20210310102318653"></p>
<ul>
<li>잘 군집되어 있음을 확인할 수 있으며 픽셀단위로 학습했음에도 불구하고 포즈도 다르고 방향도 다른 이미지들이 제대로 보여지는 것을 확인할 수 있다.</li>
</ul>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110749526-93afd480-8284-11eb-911a-23f29b5fffac.png" alt="image-20210311140648670"></p>
<ul>
<li>각각의 이미지에 대해 feature들을 뽑아놓고 임베딩 공간에 저장해둔다.</li>
</ul>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110749434-81359b00-8284-11eb-8994-3e369a335947.png" alt="image-20210310103825771"></p>
<ul>
<li>후 입력이 들어오면, 해당 feature와 비슷하게 임베딩된 feature들을 탐색한다. 유사도 탐색으로 찾아낸 k개의 feature들의 원본 image를 가져와서 반환해준다.</li>
</ul>
<p>이 feature들은 매우 고차원 공간에 위치하므로 인간이 해석하기가 너무 어렵다.</p>
</li>
<li><p>Demensionality reduction</p>
<ul>
<li>고차원 상상 힘들기 때문에 차원축소를 통해 각 클래스마다 다른 색상으로 구분한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749435-81ce3180-8284-11eb-87b0-eda48fdbcede.png" alt="image-20210310104046674"></li>
<li>각 클래스간의 분포가 비슷한 위치에 존재한다면 해당 클래스들을 유사하게 보고 있다는 것으로 해석 가능</li>
<li>3, 8 , 5 분포가 가까움 -&gt; 경계에서 헷갈릴 수 있음</li>
</ul>
</li>
</ul>
<h2 id="Activation-investigation"><a href="#Activation-investigation" class="headerlink" title="Activation investigation"></a>Activation investigation</h2><p>레이어의 activation을 분석하여 모델의 특성을 파악하는 방법</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749438-8266c800-8284-11eb-9c35-436ab37ae221.png" alt="image-20210310104403875"><ul>
<li>AlexNet 다섯번째 층의 138번째 채널 값을 thresholding을 통해 시각화-&gt;사람의 얼굴을 detection</li>
<li>각 hidden node들의 역할(activation)을 파악하는 방법</li>
</ul>
</li>
<li>Maximully activating patch<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749528-93afd480-8284-11eb-8d2a-1c77c03d8166.png" alt="image-20210311141249702"></li>
<li>레이어를 하나 골라서 해당 레이어에서 feature map 중 하나의 채널을 고른다.</li>
<li>해당 채널에서 가장 큰 값을 가지는 픽셀을 찾는다. 해당 픽셀의 receptive field가 되는 원본 이미지에서의 영역을 출력해본다.</li>
<li>입력 이미지 여러개에 대해 반복해보면, hidden node들이 어떤 작업을 하는지 파악할 수 있다.</li>
</ul>
</li>
</ul>
<h2 id="Class-visualization"><a href="#Class-visualization" class="headerlink" title="Class visualization"></a>Class visualization</h2><p>예제 데이터를 사용하지 않고, 네트워크가 기억하고 있는 이미지를 시각화하여 판단하는 방법</p>
<ul>
<li><p>Gradient ascent</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749440-8266c800-8284-11eb-9284-423822c3e63e.png" alt="image-20210310110456446"></li>
</ul>
</li>
<li><p>image synthesis</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749442-82ff5e80-8284-11eb-97eb-3b26f7c0ffbd.png" alt="image-20210310110547935"></li>
</ul>
<ol>
<li>아무런 이미지나 입력으로 준다</li>
<li>타겟 class의 스코어를 계산한 후 스코어를 최대화하는 방향으로 역전파를 통해 입력 이미지를 업데이트 해준다.(gradient를 더해준다)</li>
<li>점점 타겟 클래스에 맞게 필터들이 어떻게 학습했는지 알 수 있다.</li>
</ol>
</li>
</ul>
<h1 id="Model-decision-explanation"><a href="#Model-decision-explanation" class="headerlink" title="Model decision explanation"></a>Model decision explanation</h1><p>모델이 특정 입력을 어떤 각도로 바라보고 있는지 해석하는 방법</p>
<h2 id="Saliency-test"><a href="#Saliency-test" class="headerlink" title="Saliency test"></a>Saliency test</h2><p>영상이 판정되기 위한 각 영역의 중요도를 추출하는 방법</p>
<ul>
<li>occlusion map<ul>
<li>이미지를 일부 가리고 해당 클래스에 대한 스코어가 얼마일지 예측</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749443-8397f500-8284-11eb-9a09-11d03ede5613.png" alt="image-20210310110822001"></li>
</ul>
</li>
<li>via backpropagation<ul>
<li>gradient ascent 방법과 유사하다. 하지만 입력 이미지를 업데이트하는 것이 아니라, 입력 이미지의gradient를 새로운 이미지로 그린다.<ol>
<li>입력 이미지를 CNN에 넣어 class score를 얻는다.</li>
<li>backpropagation으로 입력 이미지의 gradient를 구한다.</li>
<li>gradient에 절댓값 또는 제곱을 하여 절대적인 크기(magnitude)를 구한다.<ul>
<li>어느 방향으로 바뀌는지 보다 해당 영역의 얼마나 큰 영향을 끼치는지가 중요</li>
</ul>
</li>
</ol>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749531-94486b00-8284-11eb-8719-30e9efb3a419.png" alt="image-20210311142809698"><ul>
<li>gradient가 높은 지점이 집중한 곳</li>
</ul>
</li>
</ul>
</li>
<li>Rectified unit(backward pass)<ul>
<li>위에서 단순하게 gradient만 그려냈다면, 이번에는 gradient를 그린 것보다 더 선명한 패턴을 시각화해 보고자 한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749446-84308b80-8284-11eb-963e-9673f3a2c98b.png" alt="image-20210310113229492"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749532-94e10180-8284-11eb-8ce0-2a3fdabc78f4.png" alt="image-20210311144038328"><ul>
<li>Forward pass : 음수가 0으로 마스킹</li>
<li>Backward pass:-backpropagation: 현재 값들을 기준으로 음수를 마스킹하지 않고 Forward 시에 0이하였던 unit들을 음수 마스킹</li>
<li>Backward pass-Deconvnet: Backward 시에 Forward시점의 값 기준이 아니라 Backpropagation 시점의 값을 기준으로 음수 마스킹. 마치 역방향으로 ReLU를 재적용</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749451-84c92200-8284-11eb-85ba-c57f8952ea4a.png" alt="image-20210310113352367"><ul>
<li>Guided backpropagation : Backward 시에 Forward 패턴도 마스킹하고, 현재 패턴 기준으로도 마스킹</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Class-Activation-mapping"><a href="#Class-Activation-mapping" class="headerlink" title="Class Activation mapping"></a>Class Activation mapping</h2><ul>
<li>CAM<ul>
<li>Heatmap을 이용하여 분석하는 방법</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749453-8561b880-8284-11eb-9674-f57cc7fa4d95.png" alt="image-20210310113531264"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749455-8561b880-8284-11eb-8b1c-2c9b2b26cf4e.png" alt="image-20210310113546947"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749458-85fa4f00-8284-11eb-9333-f992bf533fdc.png" alt="image-20210310113613397"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749460-85fa4f00-8284-11eb-92c4-01f65897c7bf.png" alt="image-20210310113830111"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749461-8692e580-8284-11eb-943c-d2168394d82a.png" alt="image-20210310113851709"></li>
</ul>
</li>
<li>Grad-CAM<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749463-872b7c00-8284-11eb-9a4c-aaf626162be1.png" alt="image-20210310114035791"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749464-872b7c00-8284-11eb-9d98-7975ebbed619.png" alt="image-20210310114049349"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749465-87c41280-8284-11eb-9d0e-12fc0dc7bf03.png" alt="image-20210310114107077"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749467-885ca900-8284-11eb-8ee7-1e7504f5314c.png" alt="image-20210310114135696"></li>
</ul>
</li>
<li>SCOUTER<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749468-88f53f80-8284-11eb-85c9-d3e5d3b17c96.png" alt="image-20210310114219576"></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749470-898dd600-8284-11eb-85db-0ac6de6513ce.png" alt="image-20210310114237549"></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-25T04:51:41.000Z" title="2021-1-25 1:51:41 ├F10: PM┤">2021-01-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-01-26T06:21:35.140Z" title="2021-1-26 3:21:35 ├F10: PM┤">2021-01-26</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/Pytorch/">Pytorch</a></span><span class="level-item">7 minutes read (About 1038 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/25/Pytorch/Pytorch1/">Pytorch1</a></h1><div class="content"><h1 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h1><h2 id="exercise-y-x-2w-2-xw-1-b-y"><a href="#exercise-y-x-2w-2-xw-1-b-y" class="headerlink" title="exercise) $y=x^2w_2+xw_1+b-y$"></a>exercise) $y=x^2w_2+xw_1+b-y$</h2><h2 id="Using-pytorch"><a href="#Using-pytorch" class="headerlink" title="Using pytorch"></a>Using pytorch</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;torch._C.Generator at 0x2cdbae1d070&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x_train = torch.FloatTensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_train = torch.FloatTensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br><span class="line">w1 = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x*x*w2 + x*w1 + b</span><br><span class="line">print(forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([0.], grad_fn=&lt;AddBackward0&gt;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 설정</span></span><br><span class="line">optimizer = optim.SGD([w2,w1,b], lr=<span class="number">2</span>*<span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line">nb_epochs = <span class="number">2000</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(nb_epochs + <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># H(x) 계산</span></span><br><span class="line">    hypothesis = x_train*x_train*w2+x_train*w1+b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cost 계산</span></span><br><span class="line">    cost = torch.mean((hypothesis - y_train) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cost로 H(x) 개선</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    cost.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">&#x27;Epoch &#123;:4d&#125;/&#123;&#125; w1: &#123;:.3f&#125; w2: &#123;:.3f&#125;  b: &#123;:.3f&#125; Cost: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">            epoch, nb_epochs, w1.item(), w2.item(), b.item(), cost.item()</span><br><span class="line">        ))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch    0/2000 w1: 0.373 w2: 0.960  b: 0.160 Cost: 18.666666
Epoch  100/2000 w1: 0.858 w2: 0.302  b: 0.828 Cost: 0.025777
Epoch  200/2000 w1: 0.959 w2: 0.255  b: 0.891 Cost: 0.014748
Epoch  300/2000 w1: 0.998 w2: 0.243  b: 0.875 Cost: 0.013746
Epoch  400/2000 w1: 1.029 w2: 0.235  b: 0.850 Cost: 0.012942
Epoch  500/2000 w1: 1.058 w2: 0.228  b: 0.825 Cost: 0.012187
Epoch  600/2000 w1: 1.086 w2: 0.221  b: 0.801 Cost: 0.011475
Epoch  700/2000 w1: 1.113 w2: 0.215  b: 0.777 Cost: 0.010806
Epoch  800/2000 w1: 1.139 w2: 0.209  b: 0.754 Cost: 0.010175
Epoch  900/2000 w1: 1.165 w2: 0.202  b: 0.732 Cost: 0.009581
Epoch 1000/2000 w1: 1.189 w2: 0.196  b: 0.710 Cost: 0.009022
Epoch 1100/2000 w1: 1.214 w2: 0.191  b: 0.689 Cost: 0.008495
Epoch 1200/2000 w1: 1.237 w2: 0.185  b: 0.669 Cost: 0.008000
Epoch 1300/2000 w1: 1.259 w2: 0.179  b: 0.649 Cost: 0.007533
Epoch 1400/2000 w1: 1.281 w2: 0.174  b: 0.630 Cost: 0.007093
Epoch 1500/2000 w1: 1.303 w2: 0.169  b: 0.611 Cost: 0.006679
Epoch 1600/2000 w1: 1.323 w2: 0.164  b: 0.593 Cost: 0.006289
Epoch 1700/2000 w1: 1.343 w2: 0.159  b: 0.575 Cost: 0.005922
Epoch 1800/2000 w1: 1.363 w2: 0.154  b: 0.558 Cost: 0.005577
Epoch 1900/2000 w1: 1.382 w2: 0.150  b: 0.542 Cost: 0.005251
Epoch 2000/2000 w1: 1.400 w2: 0.145  b: 0.526 Cost: 0.004945
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([8.4512], grad_fn=&lt;AddBackward0&gt;)
</code></pre><h2 id="Manual"><a href="#Manual" class="headerlink" title="Manual"></a>Manual</h2><p><a target="_blank" rel="noopener" href="https://velog.io/@sset2323/03.-Gradient-Descent">참조</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training Data</span></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># a random guess: random value</span></span><br><span class="line">w1 = <span class="number">0</span></span><br><span class="line">w2 = <span class="number">0</span></span><br><span class="line">b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># our model forward pass</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">pow</span>(x,<span class="number">2</span>)*w2 + x*w1 + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute gradient</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">w1_gradient</span>(<span class="params">x, y, y_pred</span>):</span>  <span class="comment"># d_loss/d_w1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x * (y_pred - y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">w2_gradient</span>(<span class="params">x, y, y_pred</span>):</span>  <span class="comment"># d_loss/d_w2</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * <span class="built_in">pow</span>(x, <span class="number">2</span>) * (y_pred - y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">b_gradient</span>(<span class="params">x, y, y_pred</span>):</span>  <span class="comment"># d_loss/d_b</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * (y_pred - y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update the weights and the bias</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(<span class="params">x, y, learning_rate = <span class="number">0.02</span></span>):</span></span><br><span class="line">    <span class="keyword">global</span> w1, w2, b</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line"></span><br><span class="line">    w1_grad = w1_gradient(x, y, y_pred)</span><br><span class="line">    w1 = w1 - learning_rate * w1_grad</span><br><span class="line"></span><br><span class="line">    w2_grad = w2_gradient(x, y, y_pred)</span><br><span class="line">    w2 = w2 - learning_rate * w2_grad</span><br><span class="line"></span><br><span class="line">    b_grad = b_gradient(x, y, y_pred)</span><br><span class="line">    b = b - learning_rate * b_grad</span><br><span class="line"></span><br><span class="line"><span class="comment">#     print(&#x27;\tgrad(w1, w2, b): &#x27;, x_val, y_val, round(w1_grad, 2), round(w2_grad, 2), round(b_grad, 2))</span></span><br><span class="line">    <span class="keyword">return</span> w1, w2, b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Before training</span></span><br><span class="line">print(<span class="string">&quot;Prediction (before training)&quot;</span>,  <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        <span class="comment"># Compute derivative w.r.t to the learned weights</span></span><br><span class="line">        <span class="comment"># Compute the loss and print progress</span></span><br><span class="line">        w1, w2, b = optimize(x_val, y_val)</span><br><span class="line">        l = loss(x_val, y_val)</span><br><span class="line">    print(<span class="string">&quot;progress : &quot;</span>, epoch, <span class="string">&quot;loss = &quot;</span>, <span class="built_in">round</span>(l, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># After training</span></span><br><span class="line">print(<span class="string">&quot;Predicted score (after training)&quot;</span>,  <span class="string">&quot;4 hours of studying: &quot;</span>, forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Prediction (before training) 4 0
progress :  0 loss =  6.38
progress :  1 loss =  11.8
progress :  2 loss =  6.54
progress :  3 loss =  6.22
progress :  4 loss =  4.57
progress :  5 loss =  3.81
progress :  6 loss =  3.01
progress :  7 loss =  2.45
progress :  8 loss =  1.98
progress :  9 loss =  1.61
progress :  10 loss =  1.32
progress :  11 loss =  1.08
progress :  12 loss =  0.9
progress :  13 loss =  0.75
progress :  14 loss =  0.63
progress :  15 loss =  0.53
progress :  16 loss =  0.45
progress :  17 loss =  0.38
progress :  18 loss =  0.33
progress :  19 loss =  0.29
Predicted score (after training) 4 hours of studying:  7.71975178478036
</code></pre><h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><p><a target="_blank" rel="noopener" href="http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html">참조</a></p>
<p><img src="http://i.imgur.com/2dKCQHh.gif?1" alt="Gradient Descent Optimization Algorithms at Long Valley"></p>
<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><h2 id="Binary-Classification-using-sigmoid"><a href="#Binary-Classification-using-sigmoid" class="headerlink" title="Binary Classification using sigmoid"></a>Binary Classification using sigmoid</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 넘파이 사용</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 맷플롯립사용</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span> <span class="comment"># 시그모이드 함수 정의</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(-<span class="number">5.0</span>, <span class="number">5.0</span>, <span class="number">0.1</span>)</span><br><span class="line">y1 = sigmoid(<span class="number">0.5</span>*x)</span><br><span class="line">y2 = sigmoid(x)</span><br><span class="line">y3 = sigmoid(<span class="number">2</span>*x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y1, <span class="string">&#x27;r&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>) <span class="comment"># W의 값이 0.5일때</span></span><br><span class="line">plt.plot(x, y2, <span class="string">&#x27;g&#x27;</span>) <span class="comment"># W의 값이 1일때</span></span><br><span class="line">plt.plot(x, y3, <span class="string">&#x27;b&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>) <span class="comment"># W의 값이 2일때</span></span><br><span class="line">plt.plot([<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1.0</span>,<span class="number">0.0</span>], <span class="string">&#x27;:&#x27;</span>) <span class="comment"># 가운데 점선 추가</span></span><br><span class="line">plt.title(<span class="string">&#x27;Sigmoid Function&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>​<br><img src="https://user-images.githubusercontent.com/46857207/105807764-f1949f80-5fe9-11eb-9461-0ea667417252.png" alt="png"><br>​</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> tensor</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> sigmoid</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># Training data and ground truth</span></span><br><span class="line">x_data = tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>], [<span class="number">4.0</span>]])</span><br><span class="line">y_data = tensor([[<span class="number">0.</span>], [<span class="number">0.</span>], [<span class="number">1.</span>], [<span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the constructor we instantiate nn.Linear module</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># One in and one out</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the forward function we accept a Variable of input data and we must return</span></span><br><span class="line"><span class="string">        a Variable of output data.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        y_pred = sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># our model</span></span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></span><br><span class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></span><br><span class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></span><br><span class="line">criterion = nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>/1000 | Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># After training</span></span><br><span class="line">print(<span class="string">f&#x27;\nLet\&#x27;s predict the hours need to score above 50%\n<span class="subst">&#123;<span class="string">&quot;=&quot;</span> * <span class="number">50</span>&#125;</span>&#x27;</span>)</span><br><span class="line">hour_var = model(tensor([[<span class="number">1.0</span>]]))</span><br><span class="line">print(<span class="string">f&#x27;Prediction after 1 hour of training: <span class="subst">&#123;hour_var.item():<span class="number">.4</span>f&#125;</span> | Above 50%: <span class="subst">&#123;hour_var.item() &gt; <span class="number">0.5</span>&#125;</span>&#x27;</span>)</span><br><span class="line">hour_var = model(tensor([[<span class="number">7.0</span>]]))</span><br><span class="line">print(<span class="string">f&#x27;Prediction after 7 hours of training: <span class="subst">&#123;hour_var.item():<span class="number">.4</span>f&#125;</span> | Above 50%: <span class="subst">&#123; hour_var.item() &gt; <span class="number">0.5</span>&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 0/1000 | Loss: 0.5570
Epoch 100/1000 | Loss: 0.5298
Epoch 200/1000 | Loss: 0.5114
Epoch 300/1000 | Loss: 0.4944
Epoch 400/1000 | Loss: 0.4786
Epoch 500/1000 | Loss: 0.4638
Epoch 600/1000 | Loss: 0.4499
Epoch 700/1000 | Loss: 0.4368
Epoch 800/1000 | Loss: 0.4246
Epoch 900/1000 | Loss: 0.4131

Let&#39;s predict the hours need to score above 50%
==================================================
Prediction after 1 hour of training: 0.3149 | Above 50%: False
Prediction after 7 hours of training: 0.9854 | Above 50%: True
</code></pre><h1 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h1><p><img src="https://miro.medium.com/max/3000/1*nrxtwp6rzqdFhgYh0x-eVw.png" alt="activation_function"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure></div></article></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/./img/avatar.jpg" alt="Keonwoo Choi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Keonwoo Choi</p><p class="is-size-6 is-block">blog</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/KeonwooChoi" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/KeonwooChoi"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/BoostCamp/"><span class="level-start"><span class="level-item">BoostCamp</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/AI/Pytorch/"><span class="level-start"><span class="level-item">Pytorch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-19T06:59:34.000Z">2021-03-19</time></p><p class="title"><a href="/2021/03/19/BoostCamp/Day39/">Day39</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-18T06:38:26.000Z">2021-03-18</time></p><p class="title"><a href="/2021/03/18/BoostCamp/Day38/">Day38</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-18T06:38:19.000Z">2021-03-18</time></p><p class="title"><a href="/2021/03/18/BoostCamp/Day37/">Day37</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-16T06:49:53.000Z">2021-03-16</time></p><p class="title"><a href="/2021/03/16/BoostCamp/Day36/">Day36</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-11T07:10:49.000Z">2021-03-11</time></p><p class="title"><a href="/2021/03/11/BoostCamp/Day33/">Day33</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">March 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">blog</a><p class="is-size-7"><span>&copy; 2021 Keonwoo Choi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>