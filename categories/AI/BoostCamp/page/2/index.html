<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: BoostCamp - blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="blog"><meta name="msapplication-TileImage" content="./img/favicon3.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="blog"><meta property="og:url" content="https://keonwoochoi.github.io/"><meta property="og:site_name" content="blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://keonwoochoi.github.io/img/og_image.png"><meta property="article:author" content="Keonwoo Choi"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://keonwoochoi.github.io"},"headline":"blog","image":["https://keonwoochoi.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Keonwoo Choi"},"description":""}</script><link rel="icon" href="/./img/favicon3.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-10-tablet is-10-desktop is-10-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/AI/">AI</a></li><li class="is-active"><a href="#" aria-current="page">BoostCamp</a></li></ul></nav></div></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/18/BoostCamp/Day38/"><img class="fill" src="/img/boostcamp.png" alt="Day38"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-18T06:38:26.000Z" title="2021-3-18 3:38:26 ├F10: PM┤">2021-03-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-18T06:41:52.164Z" title="2021-3-18 3:41:52 ├F10: PM┤">2021-03-18</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">9 minutes read (About 1339 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/18/BoostCamp/Day38/">Day38</a></h1><div class="content"><h1 id="Acceleration"><a href="#Acceleration" class="headerlink" title="Acceleration"></a>Acceleration</h1><ul>
<li>대기시간(Latency)은 요청한 데이터가 도달할 때까지 걸리는 시간이다.</li>
<li>대역폭(Bandwidth)은 단위시간 동안 전달할 수 있는 데이터의 최대치이다.</li>
<li>처리량(Throughput)은 단위시간 동안 실제로 전달되는 데이터의 양으로, 병렬 처리(Parallel processing)은 처리량을 늘리기 위한 것이다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111583668-1c3bf100-8800-11eb-88c2-79ab98f52be6.png" alt="image-20210318014304499"></li>
<li>GPU를 사용하는 이유, 병렬 처리가 가능한 라이브러리를 사용하는 이유, 모두 <strong>가속화</strong>와 밀접한 관련이 있다.</li>
</ul>
<h2 id="Hardware-acceleration"><a href="#Hardware-acceleration" class="headerlink" title="Hardware acceleration"></a>Hardware acceleration</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583669-1c3bf100-8800-11eb-8ec0-9504ab822a25.png" alt="image-20210318103611350"></li>
<li>SoC(System on Chip)는 CPU, GPU 등 다양한 시스템의 구성요소를 칩 하나에 집약한 것으로, 이를 사용하는 대표적인 제품에는 Apple의 M1이 있다.</li>
<li>FPGA(Field-programmable gate array)는 사용자가 원하는 용도에 따라 내부 회로를 코딩해서 바꿔 사용할 수 있는 칩이다.</li>
</ul>
<h3 id="공간-복잡도와-관련있는-압축은-소프트웨어에서-해결하고-시간-복잡도와-관련있는-가속은-하드웨어에서-해결"><a href="#공간-복잡도와-관련있는-압축은-소프트웨어에서-해결하고-시간-복잡도와-관련있는-가속은-하드웨어에서-해결" class="headerlink" title="공간 복잡도와 관련있는 압축은 소프트웨어에서 해결하고, 시간 복잡도와 관련있는 가속은 하드웨어에서 해결"></a>공간 복잡도와 관련있는 압축은 소프트웨어에서 해결하고, 시간 복잡도와 관련있는 가속은 하드웨어에서 해결</h3><h2 id="Deep-learning-compiler"><a href="#Deep-learning-compiler" class="headerlink" title="Deep learning compiler"></a>Deep learning compiler</h2><ul>
<li><p>딥러닝 모델을 특정 디바이스에서 효율적으로 동작시키기 위해서는 해당 디바이스에 최적화된 코드가 필요하다.  이러한 작업을 자동으로 지원해주는 도구가 바로 DL compiler</p>
</li>
<li><p>LLVM</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583670-1cd48780-8800-11eb-8858-22fc6e9a9c6b.png" alt="image-20210318104309130"></li>
<li>원래는  ‘언어의 종류 x 아키텍처의 종류’만큼 복수의 컴파일러가 필요하다</li>
<li>LLVM IR을 사용하면 Frontend에서 어떤 언어가 들어오던 원하는 architecture로 변환해</li>
</ul>
</li>
<li>MLIR<ul>
<li>MLIR은 LLVM의 Machine Learning 버전</li>
<li>MLIR에서는 중간에 있는 compiler들을 조립 후 통합해서 “통합 Framework”를 사용할 수 있게 만들었다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111583671-1cd48780-8800-11eb-9835-3c809b2ec075.png" alt="image-20210318104901155"></li>
</ul>
</li>
<li>DL complier에 적용하는 Hardware-specific optimizations<ul>
<li>Hardware Intrinsic Mapping</li>
<li>Memory Allocation &amp; Fetching</li>
<li>Memory Latency Hiding</li>
<li>Loop Oriented Optimization Techniques</li>
<li>Parallelization</li>
</ul>
</li>
</ul>
<h2 id="Pruning"><a href="#Pruning" class="headerlink" title="Pruning"></a>Pruning</h2><ul>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111583674-1d6d1e00-8800-11eb-91e7-2e02a8d736cd.png" alt="image-20210318105045016"></p>
</li>
<li><p>Pruning이란 중요하지 않고 반복되는 부분을 잘라내는 것을 뜻한다. 이를 통해 정보의 손실은 일어나겠지만, 모델의 복잡도를 줄여 일반화 성능을 높이고 속도를 높이는 효과를 낸다.</p>
</li>
<li>딥러닝에서는 Pruning을 통해 중요하지 않은 것(weight이 0에 가까운 것)을 줄인다.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583675-1d6d1e00-8800-11eb-8285-6890580eca16.png" alt="image-20210318105106354"></li>
</ul>
</li>
<li>Pruning과 Dropout은 비슷해보이지만, Pruning은 Dropout과 달리 버린 부분을 되살릴 수 없다. 또한 Dropout은 학습 시 사용하는 뉴런의 조합의 바꿔서 앙상블 효과를 낼 뿐 테스트를 할 때는 모든 뉴런을 전부 사용한다.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583676-1e05b480-8800-11eb-9380-6f0d8c4a2872.png" alt="image-20210318105138679"></li>
</ul>
</li>
<li>L1, L2 regularization<ul>
<li>모델 과적합을 방지하기 위한 방법 중 하나도 weight regularization이 있다.</li>
<li>loss function 에 weight L1 norm, L2 norm 항을 추가한 것을 각각 L1, L2 regulation 이라 칭한다.</li>
<li>pruning 비율에 따른 accuracy 감소 비율<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583678-1e05b480-8800-11eb-928e-4724bd9a32ff.png" alt="image-20210318113255066"></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111583679-1e9e4b00-8800-11eb-9a07-4a8542db5985.png" alt="image-20210318122725337"><ul>
<li>pruning 후 retrain 할 때 weight regularization 항을 추가하여 과적합(파란점)을 방지(빨간점)한다. </li>
</ul>
</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111583681-1e9e4b00-8800-11eb-8e13-6b78230e8c34.png" alt="image-20210318122805894"><ul>
<li>pruning 하면 parameter 수가 감소하고, accuracy도 낮아진다. 동일 accuracy를 보일 경우 속도는 느려진다.  (tradeoff)</li>
</ul>
</li>
</ul>
<h1 id="Pruning-category"><a href="#Pruning-category" class="headerlink" title="Pruning category"></a>Pruning category</h1><p><img src="https://user-images.githubusercontent.com/46857207/111583682-1f36e180-8800-11eb-916b-0e852787c2bd.png" alt="image-20210318123052587"></p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583684-1f36e180-8800-11eb-9919-57ad6d543d89.png" alt="image-20210318123146640"><ul>
<li>Unstructured Pruning : weight를 아무 규격 없이 잘라내는 pruning</li>
<li>Structured Pruning : weight를 channel / layer 단위로 규격을 잡아 제거하는 pruning, 규격을 잡아 제거했기 때문에 hardward optimization이 잘 된다.</li>
</ul>
</li>
</ul>
<h2 id="Iterative-pruning"><a href="#Iterative-pruning" class="headerlink" title="Iterative pruning"></a><strong>Iterative pruning</strong></h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583689-1fcf7800-8800-11eb-9164-b95e4ac5fcea.png" alt="image-20210318124855689"><ul>
<li>Pruning을 한 번에 많이 수행하게 되면, 많은 weight가 사라지게되어 pruning loss가 줄어든 후 Accuracy가 다시 올라가지 않아서 Iterative (반복적)으로 수행</li>
<li>pruning loss -&gt; retraining -&gt; pruning loss -&gt; retraining …</li>
</ul>
</li>
</ul>
<h2 id="Lottery-ticket-hypothesis"><a href="#Lottery-ticket-hypothesis" class="headerlink" title="Lottery ticket hypothesis"></a>Lottery ticket hypothesis</h2><ul>
<li><p><img src="https://user-images.githubusercontent.com/46857207/111583691-20680e80-8800-11eb-8cb2-9a9cf4680133.png" alt="image-20210318125107838"></p>
</li>
<li><p>pruning하기 전 얻었던 Accuracy (91%)를 pruning하고 나서도 얻을 수 있는 subnetwork가 원래 network 안에 존재할 것이다라는 가설</p>
</li>
<li>original network의 initialization된 parameter를 그대로 사용해야 Accuracy가 잘 나오는 subnetwork.</li>
<li>한계점은 subnetwork를 찾기 위해서는 어짜피 original network를 train해서 찾아야 한다는 것</li>
<li>한계점을 극복하기 위해 차후에 나온 방법(적은 cost)</li>
<li>Iterative Magnitude pruning<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583693-20680e80-8800-11eb-90fd-9c269f4f9c2f.png" alt="image-20210318132437123"></li>
<li>weight의 크기 기준으로 정렬해서 크기가 낮은 기준으로 잘라낸다는 의미</li>
<li>prune-&gt;mask를 씌우고 구조만 가지고 와서 다시 init</li>
</ul>
</li>
<li>Iterative Magnitude Pruning with Rewinding<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111583694-2100a500-8800-11eb-8dd8-1b5ae6a5ca7f.png" alt="image-20210318132705337"></li>
<li>k번까지만 train 시킨 후, 이때의 네트워크를 저장</li>
<li>이후 수렴할 때 까지 훈련하고 pruning한 후 마스크를 적용해준다.</li>
<li>Retrain 시, 마스크가 적용된 네트워크를 k번 학습하고 저장해놓은 네트워크를 이용하여 초기화해준다. </li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/18/BoostCamp/Day37/"><img class="fill" src="/img/boostcamp.png" alt="Day37"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-18T06:38:19.000Z" title="2021-3-18 3:38:19 ├F10: PM┤">2021-03-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-21T10:39:21.291Z" title="2021-3-21 7:39:21 ├F10: PM┤">2021-03-21</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">8 minutes read (About 1148 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/18/BoostCamp/Day37/">Day37</a></h1><div class="content"><h2 id="Space-Time-Trade-off"><a href="#Space-Time-Trade-off" class="headerlink" title="Space-Time Trade-off"></a>Space-Time Trade-off</h2><ul>
<li>공간(space)과 시간(time)은 서로 trade-off 관계에 있음.<ul>
<li>압축되지 않은(uncompressed) 데이터를 저장한다면 더 많은 공간이 필요한 대신, 더 적은 시간이 걸림.</li>
<li>데이터를 압축(compressed)해 저장한다면 더 적은 공간이 필요한 대신, 더 많은 시간이 걸림.</li>
</ul>
</li>
<li>공간(space)은 problem space(state space)와 search space(solution space)로 이루어져 있음.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901700-e8b9ca80-8a7c-11eb-94c3-758edb66f576.png" alt="image-20210317123006768"><ul>
<li>problem space는 문제를 해결하는 과정에 존재하는 모든 요소들의 공간이며, search space는 문제의 조건을 만족하는 요소들의 공간</li>
<li>search space: 모든 노드가 연결되어 있고, cycle이 존재하지 않은 그래프. 정답 후보.</li>
<li>optimal solution: 그 중 1개(또는 일부)의 cost가 가장 작은 optimal solution. 정답.</li>
</ul>
</li>
<li>시간과 공간의 제약 하에 주어진 문제(problem space)를 풀기 위해 제약 조건을 만족하는 집합(search space; feasible space) 중 가장 높은 performance를 내는 것을 찾음.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901701-e9526100-8a7c-11eb-808c-85153d3bda54.png" alt="image-20210317123048415"></li>
</ul>
</li>
</ul>
<h2 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h2><ul>
<li>불확실성의 측정</li>
<li>정렬된 왼쪽 상태를 Low Entropy, 불규칙한 오른쪽 상태<strong>를 </strong>High Entropy</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901702-e9eaf780-8a7c-11eb-8346-27c5880e5f32.png" alt="image-20210317124127726"><ul>
<li>문제해결도 엔트로피(entropy) 관점에서 볼 수 있음.</li>
<li>문제를 푼다는 특정한 initial state에서 무질서한 problem solving 과정을 통해 엔트로피가 낮은 최적의 terminal state에 도달하는 것.</li>
</ul>
</li>
</ul>
<h2 id="Parameter-search-amp-Hyper-parameter-search"><a href="#Parameter-search-amp-Hyper-parameter-search" class="headerlink" title="Parameter search &amp; Hyper-parameter search"></a>Parameter search &amp; Hyper-parameter search</h2><h3 id="Parameter-Search"><a href="#Parameter-Search" class="headerlink" title="Parameter Search"></a>Parameter Search</h3><ul>
<li>모델이 학습하며 가중치(weight)를 찾아감.<ul>
<li>class를 가장 잘 나누는 최적의 결정경계(decision boundary) 탐색.</li>
<li>loss가 커지거나 작아지는 일종의 binary search 문제. 단, global minima 보장하지 않음.</li>
</ul>
</li>
</ul>
<h3 id="Hyperparameter-Search"><a href="#Hyperparameter-Search" class="headerlink" title="Hyperparameter Search"></a>Hyperparameter Search</h3><ul>
<li><p>사람이 찾아주는 값.</p>
</li>
<li><p>한번의 hyperparameter 탐색에 cost가 너무 큼.</p>
</li>
<li><p>적은 cost를 보장하는 hyperparameter를 탐색 최적화 알고리즘 필요.</p>
</li>
<li><p><strong>Grid Search</strong></p>
<ul>
<li>hyperparameter들의 후보를 골라 조합 탐색.</li>
<li>선택한 grid의 중간 단계는 탐색하지 않음.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901703-e9eaf780-8a7c-11eb-95a2-5f3f9f2eb20d.png" alt="image-20210317124753800"></li>
</ul>
</li>
<li><p><strong>Random Search</strong></p>
<ul>
<li>범위 내의 랜덤한 값을 탐색.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901705-ea838e00-8a7c-11eb-85a7-cf9d8e218fa9.png" alt="image-20210317124812006"></li>
</ul>
<p><strong>Bayesian Optimization</strong></p>
<ul>
<li>Surrogate model(대리모델)을 만들어 학습해가며 최적의 hyperparameter 조합 탐색.<ul>
<li>Surrogate Model은 ML 모델을 정의하는 hyperparameter들을 위한 머신러닝 모델</li>
</ul>
</li>
<li>반복연산으로 cost가 크고 모델의 hyperparameter도 찾아야 함.</li>
<li>Surrogate의 대표적인 process가 Gaussian process<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901706-ea838e00-8a7c-11eb-8e89-e04f32d02ec9.png" alt="image-20210317124842798"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/111901708-eb1c2480-8a7c-11eb-9208-0d39287d002f.png" alt="image-20210317124906390"><ul>
<li>파란색 음영 ⇒ covariance</li>
<li>초록색 함수 ⇒ acquisition function</li>
<li>검정색 함숫값도 크고, 파란색 음영도 작은 부분을 찾겠다는 것.</li>
<li>exploitation은 오른쪽 점 근처에 최적값이 존재할 것”이라고 예측</li>
<li>exploration은 표준편차가 가장 큰 점, 즉, 불확실성이 가장 높은 점 근처에 최적값이 존재하는 것이라고 예측</li>
<li>Mean과 Variance를 둘 다 고려해서 Acquisㅑtion max를 찾는다</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Neural-Architecture-Search-NAS"><a href="#Neural-Architecture-Search-NAS" class="headerlink" title="Neural Architecture Search(NAS)"></a><strong>Neural Architecture Search(NAS)</strong></h2><ul>
<li>여러 모델들의 Architecture들을 알고리즘, 딥러닝 등의 모델에 넣어서 가장 좋은 성능의 Architecture를 찾아내는 방법</li>
<li><strong>Search strategy</strong><ul>
<li>어디와 어디를 residual connection,fuse,depth-wise-seperate, 몇개의 layer..</li>
<li>grid search</li>
</ul>
</li>
<li>MnasNet</li>
<li>PROXYLESSNAS</li>
<li>ONCE-FOR-ALL</li>
</ul>
<h2 id="압축-Compression-amp-압축률-Compression-rate"><a href="#압축-Compression-amp-압축률-Compression-rate" class="headerlink" title="압축 (Compression) &amp; 압축률 (Compression rate)"></a>압축 (Compression) &amp; 압축률 (Compression rate)</h2><ul>
<li><p>압축은 손실 압축과 비손실 압축으로 나뉘는데, 손실 압축은 압축된 데이터를 복원할 때 손실이 일어나지만 더 높은 압축률을 가진다.</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901711-ebb4bb00-8a7c-11eb-9bdf-4faf5aeb81e2.png" alt="image-20210317125415127"></li>
</ul>
</li>
<li><p>Entropy 관점의 Encoding</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901713-ec4d5180-8a7c-11eb-8f4a-b17da321ad04.png" alt="image-20210317151456034"></li>
<li>Cross-entropy 부분은 Q(i)라는 codebook의 입장에서 P(i)라는 message(정답 set)를 encoding 했을 때의 나오는 무질서도를 의미</li>
<li>Entropy 부분은 위에서 주황색 식 H(p)H(p), 즉, P(i)라는 message를 P(i)라는 codebook으로 encoding 했을 때를 무질서도를 의미</li>
<li>이때 entropy의 값은 0이 나오지는 않고, P(i) 분포가 기본적으로 가지고 있는 minimum entropy가 나옴</li>
<li>Cross-entropy에서 Entropy를 뺸다는 의미</li>
</ul>
</li>
<li><p>압축 후의 모델 사이즈와 정확도 손실 측면에서 봤을 때, Pruning과 Quantization을 동시에 해주는 것이 좋은 결과를 낸다.</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/111901712-ec4d5180-8a7c-11eb-99dd-3eac78749056.png" alt="image-20210317125509350"></li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/16/BoostCamp/Day36/"><img class="fill" src="/img/boostcamp.png" alt="Day36"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-16T06:49:53.000Z" title="2021-3-16 3:49:53 ├F10: PM┤">2021-03-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-16T06:51:56.814Z" title="2021-3-16 3:51:56 ├F10: PM┤">2021-03-16</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">7 minutes read (About 1085 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/16/BoostCamp/Day36/">Day36</a></h1><div class="content"><h2 id="연역적-deductive-결정"><a href="#연역적-deductive-결정" class="headerlink" title="연역적 (deductive) 결정"></a>연역적 (deductive) 결정</h2><ul>
<li>가설 증명을 통해 현상을 추론</li>
<li>“전제가 참이면, 결론도 참”이라는 논리로 결정</li>
<li>전제에 따라 결과가 바뀜</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/46857207/111267316-1ebb1100-866f-11eb-8087-29b8730e952f.png" alt="image-20210315091729505"></p>
<h2 id="귀납적-inductive-결정"><a href="#귀납적-inductive-결정" class="headerlink" title="귀납적 (inductive) 결정"></a>귀납적 (inductive) 결정</h2><ul>
<li>가설을 경험적 자료와 비교해 추론</li>
<li>높은 확률로 참, 낮은 확률로 거짓</li>
<li>머신러닝(결정기)도 이에 해당</li>
</ul>
<h2 id="결정기"><a href="#결정기" class="headerlink" title="결정기"></a>결정기</h2><ul>
<li>결정기는 어떤 데이터를 가지고 최종 판단을 내리는 것을 말한다.</li>
<li>추천시스템과 같은 가벼운 의사결정부터 암 진단과 같은 무거운 결정까지 다양</li>
<li>딥러닝이 발전하기 전에는 모델의 성능이 좋지 않았기 때문에 가벼운 결정만 가능하였지만 발전 이후 정확도가 거의 100%에 가까워지면서 무거운 의사결정가지 가능</li>
</ul>
<h2 id="가벼운-결정기"><a href="#가벼운-결정기" class="headerlink" title="가벼운 결정기"></a>가벼운 결정기</h2><p>머신러닝(결정기)은 인간의 결정을 대신 해줌</p>
<p>서비스하기 위해 큰 데이터로 학습한 모델을 경량화해서 edge device에 탑재.</p>
<ul>
<li><code>경량화</code>: 필요한 것만 갖고 불필요한 것을 제거하여 규모를 줄이거나 가볍게 만드는 것</li>
<li><code>소형화</code>: 필요/불필요한 것을 구분하지 않고 규모를 줄이거나 가볍게 만드는 것</li>
</ul>
<h2 id="무엇을-경량화"><a href="#무엇을-경량화" class="headerlink" title="무엇을 경량화?"></a>무엇을 경량화?</h2><ul>
<li>모델이 가진 무의미한 정보량을 줄여서 경량화를 달성하는 것이 목표</li>
<li>무의미한 정보가 줄어들면, 정보의 밀도가 올라간다</li>
<li>모델 압축(Compression)과도 같다</li>
</ul>
<h2 id="어떻게-경량화"><a href="#어떻게-경량화" class="headerlink" title="어떻게 경량화?"></a>어떻게 경량화?</h2><ul>
<li>Pruning</li>
<li>Quantization</li>
<li>Knowledge Distillation</li>
<li>Filter Decomposition</li>
</ul>
<h2 id="모델-경량화-과정"><a href="#모델-경량화-과정" class="headerlink" title="모델 경량화 과정"></a>모델 경량화 과정</h2><p><img src="https://user-images.githubusercontent.com/46857207/111267389-32667780-866f-11eb-990a-0ef1fa9fcd83.png" alt="image-20210316125544301"></p>
<h2 id="Edge-device"><a href="#Edge-device" class="headerlink" title="Edge device"></a>Edge device</h2><p> cloud service나 on-premise(자체 서버)를 했을 때에 엄청난 비용이 발생</p>
<p>사생활 문제와 항상 네트워크에 연결되어 있어야 한다는 문제</p>
<p><img src="https://user-images.githubusercontent.com/46857207/111267326-211d6b00-866f-11eb-8ced-1b64c5dc8f40.png" alt="image-20210315094052051"></p>
<ul>
<li>Low cost</li>
<li>No privacy concerns</li>
<li>Stand-alone</li>
</ul>
<h2 id="Cloud-intelligence-edge-intelligence"><a href="#Cloud-intelligence-edge-intelligence" class="headerlink" title="Cloud intelligence, edge intelligence"></a>Cloud intelligence, edge intelligence</h2><p><img src="https://user-images.githubusercontent.com/46857207/111267332-224e9800-866f-11eb-8373-d76098b56d67.png" alt="image-20210315094300693"></p>
<p><img src="https://user-images.githubusercontent.com/46857207/111267335-22e72e80-866f-11eb-8771-0d8105241f9e.png" alt="image-20210315094420318"></p>
<ul>
<li>Edge intellignece는 Centralized intelligence와 비교</li>
<li>좌측은 중앙 서버의 과중이 심한 반면 우측의 Edge intelligence의 경우 비교적 중앙 서버가 과중이 심하지 않음</li>
<li>Edge intelligence에는 Edge Training, Edge Offloading, Edge Caching, Edge Inference 등이 있다.</li>
<li>우리가 많이 다루게 될 내용은 Edge Inference </li>
<li>Edge Inference를 적용하기 위해서는 Pytorch로 모델을 만든 뒤 Edge device에서 사용할 수 있다 </li>
</ul>
<p><img src="https://user-images.githubusercontent.com/46857207/111267392-32667780-866f-11eb-8780-583471cd7a86.png" alt="image-20210316130537701"></p>
<ul>
<li>Edge Inferenced의 depth </li>
<li>High level에서 사용하는 PyTorch나 Tensorflow 등으로 Model을 대부분 만들고  이것들이 Edge devices로 내려가기 위해서는 Low level IR로 <strong>Graph lowering</strong></li>
</ul>
<h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><p><strong>Computer Optimization</strong></p>
<ul>
<li><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbKwed4%2FbtqZ3eTdRT0%2FmcnaZL0KdrPGjdQuryVKL0%2Fimg.gif" alt="Computer Optimization"></li>
<li>컴퓨터가 문제를 푸는 과정</li>
<li>모든 combination을 고려하기 전까지 최적해를 알 수 없음. 반대로, 유한한 모든 combination을 고려하면 무조건 최적해 보장</li>
</ul>
<p><strong>ML Optimazation</strong></p>
<ul>
<li><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FoFGNM%2FbtqZ1I1Cuyt%2FikKgKui9N5mrT1dZAjdZuK%2Fimg.gif" alt="ML"></li>
<li>머신러닝이 문제를 푸는 과정</li>
<li>모든 상태를 고려하지 않고 현재 상태보다 낫게 업데이트해 최적화</li>
</ul>
<h1 id="Decision-problem-vs-optimization-problem"><a href="#Decision-problem-vs-optimization-problem" class="headerlink" title="Decision problem vs optimization problem"></a>Decision problem vs optimization problem</h1><h2 id="Decision-problem"><a href="#Decision-problem" class="headerlink" title="Decision problem"></a>Decision problem</h2><ul>
<li>조건을 만족한다면 문제를 푼 것. 하나의 결정.</li>
<li>무한한 자원으로 최고 성능을 내는 것.</li>
<li>Decision Spanning Tree (DST)<ul>
<li>upper bound가 정해져 있고, 그 upper bound만 만족하면 풀리는 문제</li>
</ul>
</li>
</ul>
<h2 id="Optimazation-problem"><a href="#Optimazation-problem" class="headerlink" title="Optimazation problem"></a>Optimazation problem</h2><ul>
<li>더이상 못찾을 때까지 decision을 연쇄적으로 반복.</li>
<li>제약 조건을 만족하면서 최고의 성능을 내는 것<ul>
<li>Minimum Spanning Tree(MST)</li>
<li>Dicision problem을 연쇄적으로 반복했을 때 해결</li>
</ul>
</li>
</ul>
<h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints"></a><strong>Constraints</strong></h2><p>무엇을 원하느냐에 따라 굉장히 달라짐(시간,돈…)</p>
<p><img src="https://user-images.githubusercontent.com/46857207/111267395-32ff0e00-866f-11eb-95ec-6155326423ff.png" alt="image-20210316133450139"></p>
<ul>
<li>Decision problem에서 무한한 자원 (infinite amount of resoources)을 사용한다고 가정</li>
<li>Optimization problem에서는 각각의 Decision problem에서 지불했던 Cost를 더해서 Constraints를 계산합니다. 이때, 모든 Cost를 더했을 때 Constraint를 넘으면 안된다</li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/11/BoostCamp/Day33/"><img class="fill" src="/img/boostcamp.png" alt="Day33"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-11T07:10:49.000Z" title="2021-3-11 4:10:49 ├F10: PM┤">2021-03-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-11T07:21:20.429Z" title="2021-3-11 4:21:20 ├F10: PM┤">2021-03-11</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">24 minutes read (About 3569 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/11/BoostCamp/Day33/">Day33</a></h1><div class="content"><h1 id="Object-detection"><a href="#Object-detection" class="headerlink" title="Object detection"></a>Object detection</h1><h2 id="What-is-object-detection"><a href="#What-is-object-detection" class="headerlink" title="What is object detection"></a>What is object detection</h2><ul>
<li>semantic segmentation보다 더 구체적</li>
<li>객체가 달라도 구분이 됨</li>
<li>Instance segmentation ⊂ Panoptic sementation</li>
<li>object detection=Classification + Box localization<ul>
<li>two-stage: box localization → classfication</li>
<li>one-stage: box + classfication 한번에</li>
</ul>
</li>
</ul>
<h2 id="What-are-the-applications"><a href="#What-are-the-applications" class="headerlink" title="What are the applications"></a>What are the applications</h2><ul>
<li>autonomous driveing</li>
<li>ocr</li>
</ul>
<h1 id="Two-stage-detector"><a href="#Two-stage-detector" class="headerlink" title="Two-stage detector"></a>Two-stage detector</h1><h2 id="Traditional"><a href="#Traditional" class="headerlink" title="Traditional"></a>Traditional</h2><ul>
<li>Gradient-based detector<ul>
<li>과거에는 경계선을 특징으로 모델링 , gradient 방향성을 활용한 선형 모델 사용</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749394-78dd6000-8284-11eb-8939-33f4b781b27e.png" alt="image-20210310091730224"></li>
</ul>
</li>
<li>selective search<ul>
<li>사람이나 특정 물체 뿐만 아니라, 다양한 물체 후보군의 영역 후보군을 지정해주는 방식</li>
<li>bounding box 제안-&gt; Proposal Algorithm</li>
<li>Over-segmentation: 영상을 색끼리 잘게 분할</li>
<li>merginig: 비슷한 feature를 가지는 영역들을 합침</li>
<li>반복해서 합쳐주다보면 object를 소수로 특정지음</li>
<li>특정지은 소수의 object 위치를 바운딩 박스로 나타냄</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749396-7975f680-8284-11eb-9a67-3e54ab12e29b.png" alt="image-20210310092048339"></li>
</ul>
</li>
</ul>
<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><ul>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110749518-927ea780-8284-11eb-8b6b-19c497d8f97d.png" alt="image-20210311104814315"></p>
</li>
<li><p>Selective Search를 통해 Region Proposal을 진행 → 2K 이하로 설정</p>
</li>
<li>각 Region Proposal을 CNN의 input size로 Warping</li>
<li>기존에 pre-trained된 CNN에 input으로 넣어서 Classification을 진행</li>
<li>SVM의 linear classifier만을 이용해서 클래스를 학습(fine-tuning)</li>
<li>단점<ul>
<li>모든 region proposal이 CNN에 입력값으로 들어가기 때문에 느림</li>
<li>Hand Designed된 selective search-&gt; 학습을 통한 성능향상에 한계</li>
</ul>
</li>
</ul>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749399-7a0e8d00-8284-11eb-93bc-ea409f518ab2.png" alt="image-20210310092641179"></li>
<li>영상 전체에 대한 Feature을 한번에 추출하고 이를 재활용해서 object detection</li>
</ul>
<ol>
<li>Conv layer를 통해 feature map 검출<ul>
<li>conv layer를 거쳤으므로 tensor형태가 된다</li>
<li>Fully Convolutional Network는 입력사이즈와 상관없이 Feature Map을 뽑아낼 수 있기 때문에 warp 필요 없다</li>
</ul>
</li>
<li>RoI Pooling: Feature를 여러번 재활용, Region Proposal이 제시한 물체의 후보 위치들(Bounding Box)에 대해서 RoI에 해당하는 Feature만 추출한다.<ul>
<li>RoI feature를 고정된 사이즈로 resampling한다</li>
</ul>
</li>
<li>classification : softmax, bbx regression로 위치 조정</li>
</ol>
<ul>
<li>selective search를 사용했으므로 성능 향상 한계</li>
</ul>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><ul>
<li>IoU<ul>
<li>두 영역의 OVERLAP 측정</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749401-7a0e8d00-8284-11eb-8126-cf48f190ea43.png" alt="image-20210310092844050"></li>
</ul>
</li>
<li>anchor box 각 위치에서 발생할 것 같은 박스를 미리 정의해둔 후보군<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749404-7aa72380-8284-11eb-9011-51add2eb2350.png" alt="image-20210310092945166"></li>
<li>box의 개수와 종류는 hyperparameter</li>
<li>Faster R-CNN에서는 9개 (scale 3 * 비율3)</li>
</ul>
</li>
<li>selective search 대신 RPN 모듈 제안<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749406-7aa72380-8284-11eb-8f51-62d5eae5554d.png" alt="image-20210310093216117"></li>
<li>Fast R-CNN과 마찬가지로 각각의 영상에서 공유되는 Feature Map을 미리 뽑아두고, 해당 Feature Map을 바탕으로 RPN에서 Region Proposal을 여러 개 제공하고 RoI Pooling을 실시한 다음 Classifier를 통해 정답을 예측</li>
</ul>
</li>
<li>Region Proposal Network<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749407-7b3fba00-8284-11eb-81d1-43144ac30ad2.png" alt="image-20210310093418147"></li>
<li>Conv Layer를 통해 나온 Feature Map에 Sliding Window 방식으로 돌면서 매 위치마다 미리 정의해둔 K개의 Anchor Box를 고려<ol>
<li>각 위치에서 256-d의 feature vector를 하나 추출</li>
<li>각 vector로 부터 Object인지 아닌지에 대한 score인 2K개 classification score (object, non-object)</li>
<li>k개의 Bounding Box위치를 regression하는 4k 개의 Coordinates (x,y,w,h 4개)<ul>
<li>Anchor Box를 촘촘하게 만들면 계산 속도가 느려지므로 대표적인 비율과 Scale만 정해두고 정교한 위치는 Regression 문제로 분할 정복</li>
</ul>
</li>
<li>각각의 Loss → Cross Entropy Loss + Regression Loss를 사용<ul>
<li>이 2가지 Loss가 RPN을 위한 요소가 되며, 전체 Target Task를 위한 RoI별 Classification Loss는 따로 하나가 추가가 되서 전체적으로 End-to-End로 학습을 진행한다.</li>
</ul>
</li>
</ol>
</li>
<li>RPN에서 object를 완전히 class로 분류한게 아니라, object인지 아닌지만 판단</li>
<li>RPN에서 classifier, regressor 두개의 모델이 필요하고, 마지막으로 classifier가 필요하다. 또 맨 처음 feature map을 뽑아낼 CNN도 필요하다. 총 4개의 모델이 각각이 아니라 한번에 학습된다.</li>
</ul>
</li>
<li>non maximum suppression<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749409-7bd85080-8284-11eb-9a84-cfd804c87e2e.png" alt="image-20210310093530590"></li>
<li>IoU스코어가 낮은 박스들은 제거하는 방식의 NMS 알고리즘을 추가한다.</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749411-7c70e700-8284-11eb-922c-d2d10de4c07a.png" alt="image-20210310093600826"></li>
</ul>
<h1 id="Single-stage-detector"><a href="#Single-stage-detector" class="headerlink" title="Single-stage-detector"></a>Single-stage-detector</h1><h2 id="Comparison-with-two-stage-detector"><a href="#Comparison-with-two-stage-detector" class="headerlink" title="Comparison with two-stage detector"></a>Comparison with two-stage detector</h2><ul>
<li>성능을 조금 포기하고 계산 속도를 확보해서 real-time detection 이 가능하도록한 모델</li>
<li>ROI pooling을 하지 않기 때문에 구조가 매우 간단한 편</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749412-7c70e700-8284-11eb-8c45-473994cb8d33.png" alt="image-20210310093821561"></li>
</ul>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749413-7d097d80-8284-11eb-9609-7988796876ae.png" alt="image-20210310093908226"></li>
<li>input image를 SxS grid로 나눈다</li>
<li>각 grid마다 바운딩박스 정보 4개 + object여부 1개 + Class 분류 확률 C개 = 5+C개의 정보를 한꺼번에 예측한다.</li>
<li>Faster R-CNN의 RPN과 마찬가지로, ground truth와의 IoU스코어가 높은 Anchor box를 positive로 간주하여 loss 계산</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749416-7d097d80-8284-11eb-94be-293724797c46.png" alt="image-20210310094058186"><ul>
<li>30 channel= 앵커박스를 grid마다 2개만 사용했고, class 개수는 총 20개여서 5*2+20=30</li>
<li>s는 convolution layer에서의 마지막 해상도로 결정됨</li>
<li>마지막 layer에서만 prediction을 수행하기 떄문에 localization 정확도 떨어짐</li>
</ul>
</li>
</ul>
<h2 id="single-shot-multibox-detector-SSD"><a href="#single-shot-multibox-detector-SSD" class="headerlink" title="single shot multibox detector(SSD)"></a>single shot multibox detector(SSD)</h2><ul>
<li>SSD는 Multi Scale object를 더 잘 처리하기 위해서 중간 Feature를 각 해상도에 적절한 Bounding Box 들을 출력할 수 있도록하는 Multi Scale구조를 만들었다.</li>
<li>각기 다른 크기의 feature map에서 정해진 크기의 anchor box를 사용하면, feature map 사이즈에 따라 anchor box가 scaling된다.</li>
<li>feature map 사이즈마다, anchor box를 적용하여 object를 탐지한다. 때문에 YOLO에 비해 계산량이 늘어난다</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749418-7da21400-8284-11eb-9014-aac4a48e9928.png" alt="image-20210310094304705"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749420-7e3aaa80-8284-11eb-9413-98015b4b5f28.png" alt="image-20210310094408490"><ul>
<li>SSD는 각 층마다 classifier적용</li>
<li>각 layer마다 (class수+위치정보4)에 anchor box개수를 곱해준다</li>
</ul>
</li>
</ul>
<h1 id="Two-stage-vs-One-stage"><a href="#Two-stage-vs-One-stage" class="headerlink" title="Two-stage vs One-stage"></a>Two-stage vs One-stage</h1><h2 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h2><ul>
<li>일반적으로object보다 background가 더 넗음-&gt; class imbalance</li>
<li>모든 구역에 대해 object detection하는 경우 negative를 결과로 내는 anchor boxrk 많아진다.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749421-7ed34100-8284-11eb-8e38-af8862cfade9.png" alt="image-20210310094634765"></li>
</ul>
</li>
<li>focal loss= cross entropy의 확장<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749423-7ed34100-8284-11eb-9e64-9c90ba6c20cb.png" alt="image-20210310094659841"></li>
<li>파란색 그래프가 기존의 Cross Entropy</li>
<li>나머지가 FL함수에서 $\gamma$값을 바꿔주었을 때의 그래프다.</li>
<li>학률term을 통해 잘 맞춘 애들은 더 낮은 loss를 주고 맞추지 못한 애들은 더 높은 loss를 준다.</li>
<li>FL함수는 정답률이 높을수록 gradient를 0에 가깝게 만들고, 정답률이 낮을수록 gradient가 가팔라지게</li>
<li>Loss 값 자체가 중요한게 아니라 gradient</li>
<li>$\gamma$값을 크게줄수록 적용 폭이 강해진다.</li>
<li>Q) gamma가 높을수록 loss가 낮아지지만 잘 찾지 못할 때 loss는 소폭 줄어들게 하고 잘 찾은 경우에서는 loss를 대폭 줄어들게 합니다</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749520-93173e00-8284-11eb-968e-c751c79106fd.png" alt="image-20210311133620851"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749522-93173e00-8284-11eb-9964-c4b4683ea665.png" alt="image-20210311133647011"></li>
</ul>
</li>
</ul>
<h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h2><ul>
<li>FPN<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749425-7f6bd780-8284-11eb-924f-92c5964eb125.png" alt="image-20210310094941624"></li>
<li>U-Net과 비슷한 구조다.</li>
<li>residual을 활용하여 더해줌으로써, low-level의 특징과 high-level의 특징을 둘 다 활용하면서 각 scale별로 object를 잘 찾기 위한 multi-scale 구조를 갖게된다. (concat이 아닌 더하기)</li>
<li>class 분류와 box 회귀 예측을 진행한다.</li>
</ul>
</li>
</ul>
<h1 id="Detection-with-Transform"><a href="#Detection-with-Transform" class="headerlink" title="Detection with Transform"></a>Detection with Transform</h1><ul>
<li>DETR</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749426-7f6bd780-8284-11eb-9011-080928167227.png" alt="image-20210310095209241"></li>
<li>CNN으로 Feature추출 후, Positional encoding을 더해준다.</li>
<li>encoder 학습 후, decoder에 query를 입력으로 준다. (query는 object 정보이거나, positional encoding 정보) 추가로, 최대 몇개의 box를 찾아줄지에 대한 값도 함께 준다. (N)</li>
<li>해당 query에 맞는 object를 찾아 box를 찾아준다. positional encoding의 경우 해당 위치 근처의 바운딩 박스와 class를 찾아준다</li>
</ul>
<h1 id="Visualizing-CNN"><a href="#Visualizing-CNN" class="headerlink" title="Visualizing CNN"></a>Visualizing CNN</h1><h2 id="What-is-CNN-visualization"><a href="#What-is-CNN-visualization" class="headerlink" title="What is CNN visualization"></a>What is CNN visualization</h2><ul>
<li>딥러닝 네트워크= black box</li>
<li>CNN의 경우 시각화가 가능하다.</li>
<li>visualization as debugging tool</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749427-80046e00-8284-11eb-845a-face0dfb2896.png" alt="image-20210310101554404"><ul>
<li>low-level feature는 방향성이 있는 선, 동그란 블록</li>
<li>high-level로 갈수록 의미있는 feature</li>
</ul>
</li>
</ul>
<h2 id="Vanilla-example-filter-visualization"><a href="#Vanilla-example-filter-visualization" class="headerlink" title="Vanilla example: filter visualization"></a>Vanilla example: filter visualization</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749428-809d0480-8284-11eb-9807-59b73cd6e9fb.png" alt="image-20210310101738368"><ul>
<li>AlexNet 첫번째 층의 필터를 시각화</li>
<li>해당 필터를 이미지에 적용했을 때의 결과(하나의 필터만 적용했기 때문에 밝기값만 가지는 1채널 이미지)</li>
<li>높은 층으로 갈수록 필터도 채널 수가 늘어나 고차원의 필터를 시각화하기도 어렵고, 시각화 하더라도 사람이 해석할 수 없다</li>
</ul>
</li>
</ul>
<h2 id="How-to-visualize-neural-network"><a href="#How-to-visualize-neural-network" class="headerlink" title="How to visualize neural network"></a>How to visualize neural network</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749431-809d0480-8284-11eb-8320-f9de1cd8fdf5.png" alt="image-20210310102033215"><ul>
<li>왼쪽으로 갈수록 모델 자체를 분석</li>
<li>른쪽으로 갈수록 모델의 출력 결과가 왜 그렇게 나타났는지 분석</li>
</ul>
</li>
</ul>
<h1 id="Analysis-of-model-behaviors"><a href="#Analysis-of-model-behaviors" class="headerlink" title="Analysis of model behaviors"></a>Analysis of model behaviors</h1><h2 id="Embedding-feature-analysis"><a href="#Embedding-feature-analysis" class="headerlink" title="Embedding feature analysis"></a>Embedding feature analysis</h2><p>High-level layer에서 얻는 High-level feature들을 분석하는 방법</p>
<ul>
<li><p>nearest neighbors in a feature space</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110749433-81359b00-8284-11eb-82bd-232d7dd73a71.png" alt="image-20210310102318653"></p>
<ul>
<li>잘 군집되어 있음을 확인할 수 있으며 픽셀단위로 학습했음에도 불구하고 포즈도 다르고 방향도 다른 이미지들이 제대로 보여지는 것을 확인할 수 있다.</li>
</ul>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110749526-93afd480-8284-11eb-911a-23f29b5fffac.png" alt="image-20210311140648670"></p>
<ul>
<li>각각의 이미지에 대해 feature들을 뽑아놓고 임베딩 공간에 저장해둔다.</li>
</ul>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110749434-81359b00-8284-11eb-8994-3e369a335947.png" alt="image-20210310103825771"></p>
<ul>
<li>후 입력이 들어오면, 해당 feature와 비슷하게 임베딩된 feature들을 탐색한다. 유사도 탐색으로 찾아낸 k개의 feature들의 원본 image를 가져와서 반환해준다.</li>
</ul>
<p>이 feature들은 매우 고차원 공간에 위치하므로 인간이 해석하기가 너무 어렵다.</p>
</li>
<li><p>Demensionality reduction</p>
<ul>
<li>고차원 상상 힘들기 때문에 차원축소를 통해 각 클래스마다 다른 색상으로 구분한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749435-81ce3180-8284-11eb-87b0-eda48fdbcede.png" alt="image-20210310104046674"></li>
<li>각 클래스간의 분포가 비슷한 위치에 존재한다면 해당 클래스들을 유사하게 보고 있다는 것으로 해석 가능</li>
<li>3, 8 , 5 분포가 가까움 -&gt; 경계에서 헷갈릴 수 있음</li>
</ul>
</li>
</ul>
<h2 id="Activation-investigation"><a href="#Activation-investigation" class="headerlink" title="Activation investigation"></a>Activation investigation</h2><p>레이어의 activation을 분석하여 모델의 특성을 파악하는 방법</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749438-8266c800-8284-11eb-9c35-436ab37ae221.png" alt="image-20210310104403875"><ul>
<li>AlexNet 다섯번째 층의 138번째 채널 값을 thresholding을 통해 시각화-&gt;사람의 얼굴을 detection</li>
<li>각 hidden node들의 역할(activation)을 파악하는 방법</li>
</ul>
</li>
<li>Maximully activating patch<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749528-93afd480-8284-11eb-8d2a-1c77c03d8166.png" alt="image-20210311141249702"></li>
<li>레이어를 하나 골라서 해당 레이어에서 feature map 중 하나의 채널을 고른다.</li>
<li>해당 채널에서 가장 큰 값을 가지는 픽셀을 찾는다. 해당 픽셀의 receptive field가 되는 원본 이미지에서의 영역을 출력해본다.</li>
<li>입력 이미지 여러개에 대해 반복해보면, hidden node들이 어떤 작업을 하는지 파악할 수 있다.</li>
</ul>
</li>
</ul>
<h2 id="Class-visualization"><a href="#Class-visualization" class="headerlink" title="Class visualization"></a>Class visualization</h2><p>예제 데이터를 사용하지 않고, 네트워크가 기억하고 있는 이미지를 시각화하여 판단하는 방법</p>
<ul>
<li><p>Gradient ascent</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749440-8266c800-8284-11eb-9284-423822c3e63e.png" alt="image-20210310110456446"></li>
</ul>
</li>
<li><p>image synthesis</p>
<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749442-82ff5e80-8284-11eb-97eb-3b26f7c0ffbd.png" alt="image-20210310110547935"></li>
</ul>
<ol>
<li>아무런 이미지나 입력으로 준다</li>
<li>타겟 class의 스코어를 계산한 후 스코어를 최대화하는 방향으로 역전파를 통해 입력 이미지를 업데이트 해준다.(gradient를 더해준다)</li>
<li>점점 타겟 클래스에 맞게 필터들이 어떻게 학습했는지 알 수 있다.</li>
</ol>
</li>
</ul>
<h1 id="Model-decision-explanation"><a href="#Model-decision-explanation" class="headerlink" title="Model decision explanation"></a>Model decision explanation</h1><p>모델이 특정 입력을 어떤 각도로 바라보고 있는지 해석하는 방법</p>
<h2 id="Saliency-test"><a href="#Saliency-test" class="headerlink" title="Saliency test"></a>Saliency test</h2><p>영상이 판정되기 위한 각 영역의 중요도를 추출하는 방법</p>
<ul>
<li>occlusion map<ul>
<li>이미지를 일부 가리고 해당 클래스에 대한 스코어가 얼마일지 예측</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749443-8397f500-8284-11eb-9a09-11d03ede5613.png" alt="image-20210310110822001"></li>
</ul>
</li>
<li>via backpropagation<ul>
<li>gradient ascent 방법과 유사하다. 하지만 입력 이미지를 업데이트하는 것이 아니라, 입력 이미지의gradient를 새로운 이미지로 그린다.<ol>
<li>입력 이미지를 CNN에 넣어 class score를 얻는다.</li>
<li>backpropagation으로 입력 이미지의 gradient를 구한다.</li>
<li>gradient에 절댓값 또는 제곱을 하여 절대적인 크기(magnitude)를 구한다.<ul>
<li>어느 방향으로 바뀌는지 보다 해당 영역의 얼마나 큰 영향을 끼치는지가 중요</li>
</ul>
</li>
</ol>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749531-94486b00-8284-11eb-8719-30e9efb3a419.png" alt="image-20210311142809698"><ul>
<li>gradient가 높은 지점이 집중한 곳</li>
</ul>
</li>
</ul>
</li>
<li>Rectified unit(backward pass)<ul>
<li>위에서 단순하게 gradient만 그려냈다면, 이번에는 gradient를 그린 것보다 더 선명한 패턴을 시각화해 보고자 한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749446-84308b80-8284-11eb-963e-9673f3a2c98b.png" alt="image-20210310113229492"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749532-94e10180-8284-11eb-8ce0-2a3fdabc78f4.png" alt="image-20210311144038328"><ul>
<li>Forward pass : 음수가 0으로 마스킹</li>
<li>Backward pass:-backpropagation: 현재 값들을 기준으로 음수를 마스킹하지 않고 Forward 시에 0이하였던 unit들을 음수 마스킹</li>
<li>Backward pass-Deconvnet: Backward 시에 Forward시점의 값 기준이 아니라 Backpropagation 시점의 값을 기준으로 음수 마스킹. 마치 역방향으로 ReLU를 재적용</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749451-84c92200-8284-11eb-85ba-c57f8952ea4a.png" alt="image-20210310113352367"><ul>
<li>Guided backpropagation : Backward 시에 Forward 패턴도 마스킹하고, 현재 패턴 기준으로도 마스킹</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Class-Activation-mapping"><a href="#Class-Activation-mapping" class="headerlink" title="Class Activation mapping"></a>Class Activation mapping</h2><ul>
<li>CAM<ul>
<li>Heatmap을 이용하여 분석하는 방법</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749453-8561b880-8284-11eb-9674-f57cc7fa4d95.png" alt="image-20210310113531264"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749455-8561b880-8284-11eb-8b1c-2c9b2b26cf4e.png" alt="image-20210310113546947"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749458-85fa4f00-8284-11eb-9333-f992bf533fdc.png" alt="image-20210310113613397"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749460-85fa4f00-8284-11eb-92c4-01f65897c7bf.png" alt="image-20210310113830111"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749461-8692e580-8284-11eb-943c-d2168394d82a.png" alt="image-20210310113851709"></li>
</ul>
</li>
<li>Grad-CAM<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749463-872b7c00-8284-11eb-9a4c-aaf626162be1.png" alt="image-20210310114035791"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749464-872b7c00-8284-11eb-9d98-7975ebbed619.png" alt="image-20210310114049349"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749465-87c41280-8284-11eb-9d0e-12fc0dc7bf03.png" alt="image-20210310114107077"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749467-885ca900-8284-11eb-8ee7-1e7504f5314c.png" alt="image-20210310114135696"></li>
</ul>
</li>
<li>SCOUTER<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110749468-88f53f80-8284-11eb-85c9-d3e5d3b17c96.png" alt="image-20210310114219576"></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110749470-898dd600-8284-11eb-85db-0ac6de6513ce.png" alt="image-20210310114237549"></li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/10/BoostCamp/Day32/"><img class="fill" src="/img/boostcamp.png" alt="Day32"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-10T06:23:14.000Z" title="2021-3-10 3:23:14 ├F10: PM┤">2021-03-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-22T09:32:51.989Z" title="2021-3-22 6:32:51 ├F10: PM┤">2021-03-22</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">11 minutes read (About 1606 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/10/BoostCamp/Day32/">Day32</a></h1><div class="content"><h1 id="Problems-with-deeper-layers"><a href="#Problems-with-deeper-layers" class="headerlink" title="Problems with deeper layers"></a>Problems with deeper layers</h1><h2 id="Going-deeper-with-convolutions"><a href="#Going-deeper-with-convolutions" class="headerlink" title="Going deeper with convolutions"></a>Going deeper with convolutions</h2><ul>
<li>larger receptive fields</li>
<li>more capacity and non linearity</li>
<li>but gradient vanishing/exploding, computationally complex</li>
<li>degradation problem, not overfitting</li>
</ul>
<h1 id="CNN-artchitectures-for-image-classification"><a href="#CNN-artchitectures-for-image-classification" class="headerlink" title="CNN artchitectures for image classification"></a>CNN artchitectures for image classification</h1><h2 id="GoogLenet"><a href="#GoogLenet" class="headerlink" title="GoogLenet"></a>GoogLenet</h2><ul>
<li><p>여러 합성곱 및 풀링 레이어에 대한 병렬적 연산을 수행-&gt;concat</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110585642-727fb300-81b4-11eb-8f0f-ba86c6795084.png" alt="image-20210309091646305"></p>
</li>
<li>The increased network size increases the use of computational resources -&gt; 1x1 conv<ul>
<li>필터 수가 출력의 채널이 되어, 채널을 줄일 수 있게 됨</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585644-73184980-81b4-11eb-9667-108221994f06.png" alt="image-20210309091759394"></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585647-73184980-81b4-11eb-8c5f-cebb63f266ca.png" alt="image-20210309092141934"><ul>
<li>각 Inception 블록마다 단계별로 loss값을 구하는 경로를 마련</li>
<li>최종 Output에 대한 loss와 Auxiliary Classifier로부터 얻은 loss를 모두 종합하여 역전파 수행</li>
</ul>
</li>
</ul>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><ul>
<li>depth가 성능에 중요</li>
<li>overfitting이 아니라 degradation problem</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585649-73b0e000-81b4-11eb-9642-83d24d1927b5.png" alt="image-20210309093050034"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585650-73b0e000-81b4-11eb-9821-0880873bafb9.png" alt="image-20210309093132356"><ul>
<li>Skip-Connection을 하나 추가할 때마다 Gradient 전파경로의 경우의 수가 2배 증가하여, Gradient 전파에 대한 시간 복잡도는 O(2^n)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1605.06431.pdf">[참고]Residual Networks Behave Like Ensembles of Relatively Shallow Networks</a></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585653-74497680-81b4-11eb-8b65-d4659e7d30d8.png" alt="image-20210309093258843"></li>
</ul>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><ul>
<li>이전의 모든 레이어에 대한 정보들을 입력값으로 넣어줌-&gt; Concat하며 학습</li>
<li>채널이 늘어남으로 메모리도 늘어남 ,but feature 보존</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585657-757aa380-81b4-11eb-989f-a22351e490a2.png" alt="image-20210309094132892"></li>
</ul>
<h2 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h2><ul>
<li><p><strong>Squeeze &amp; Excitation Block</strong></p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110585659-757aa380-81b4-11eb-9527-52052db5ff04.png" alt="image-20210309094258516"></p>
</li>
</ul>
<h2 id="EfficientNet"><a href="#EfficientNet" class="headerlink" title="EfficientNet"></a>EfficientNet</h2><ul>
<li>성능을 높이는 방법에 따라 Saturation Point가 다름</li>
<li><p>각각의 유용한 방법들을 적절한 비율로 동시 scaling 함</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110585663-76abd080-81b4-11eb-9cf6-f671e5d36b72.png" alt="image-20210309094415737"></p>
</li>
</ul>
<h2 id="Deformable-convolution"><a href="#Deformable-convolution" class="headerlink" title="Deformable convolution"></a>Deformable convolution</h2><ul>
<li>사람과 동물같은 deformable한 형태를 고려</li>
<li><p>리드를 활용하여 객체를 유연하게 파악</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110585665-77dcfd80-81b4-11eb-9522-a8db2e012546.png" alt="image-20210309094743706"></p>
</li>
</ul>
<h1 id="Semantic-segmentation"><a href="#Semantic-segmentation" class="headerlink" title="Semantic segmentation"></a>Semantic segmentation</h1><ul>
<li>이미지 분류를 영상 단위가 아니라 픽셀 별로 하는 것. 단, 같은 클래스(종류)이면서 서로 다른 물체(개체)를 구분하지는 않는다</li>
</ul>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h2><ul>
<li>입력부터 출력까지 모두 인공신경망으로만 구성된 end-to-end 구조</li>
<li>어떤 사이즈의 이미지도 입력할 수 있고, 입력 이미지와 동일한 크기의 Segmentation 결과를 얻을 수 있음</li>
<li>Fully Connected 구조를 사용하지 않고 업샘플링을 적용하여 저해상도 문제를 해결<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110585666-77dcfd80-81b4-11eb-8fab-4fb8c8ff1bfc.png" alt="image-20210309101313587"></li>
</ul>
</li>
<li>1×1 Conv 레이어를 활용하여 Spatial Information을 유지-&gt;히트맵을 얻음<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110585668-78759400-81b4-11eb-9910-4034bf304129.png" alt="image-20210309101701902"></li>
<li>1×1 Conv 레이어를 활용하면 채널 간 압축 과정 일어남</li>
<li>이는 기존의 Feature Map을 채널을 주축으로 Flatten하여 FC 레이어를 적용하는 것과 같음</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585838-9b07ad00-81b4-11eb-902d-65550b438112.png" alt="image-20210310132814603"></li>
</ul>
</li>
</ul>
<h2 id="Upsampling"><a href="#Upsampling" class="headerlink" title="Upsampling"></a>Upsampling</h2><ul>
<li>저해상도 문제를 회피하기 위한 방법</li>
<li>Conv, Pooling 레이어를 줄일수록 receptive field가 줄어들기 떄문에 일단은 작게 만들어서 receptive field 키워서 영상의 전반적인 context 파악할 수 있게 함</li>
<li><p>이후 upsampling을 통해 강제로 해상도 맞춰줌</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110585669-790e2a80-81b4-11eb-8c7f-3021367be96e.png" alt="image-20210309102112044"></p>
</li>
<li>Tranposed convolution<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110585839-9b07ad00-81b4-11eb-9aae-8ef4ccac3448.png" alt="image-20210310133454920"></li>
<li>입력 이미지에 필터를 적용하여 사이즈를 크게 변환하는 방법</li>
<li>연산 과정에서 중첩 문제가 발생하는데 (checkboard artifact), 때문에 필터 사이즈와 Stride에 대한 튜닝이 필수적</li>
<li>overlap되는 구간은 다른 구간들보다 상대적으로 출력값이 높아 진해짐</li>
</ul>
</li>
<li>upsample and convolution<ul>
<li>interpolution과 convolution을 분리</li>
<li>upsampling을 통해 중첩 문제가 없이 골고루 영향을 받게 함</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585670-79a6c100-81b4-11eb-85fc-1264294a779d.png" alt="image-20210309102334856"></li>
</ul>
</li>
<li>low-level의 detail, local + high-leve의 global<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110585671-79a6c100-81b4-11eb-82e2-dd584518ab98.png" alt="image-20210309102435541"></li>
<li>높은 layer의 activation map을 upsampling하여 해상도를 크게 끌어올린다.</li>
<li>이에 맞추어 중간 layer의 activation map을 upsampling하여 가져오고, concat한다.</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585674-7a3f5780-81b4-11eb-8d2c-aa0fe4b79d3e.png" alt="image-20210309102546068"></li>
</ul>
<h2 id="Hypercolumns-for-object-segmentation"><a href="#Hypercolumns-for-object-segmentation" class="headerlink" title="Hypercolumns for object segmentation"></a>Hypercolumns for object segmentation</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110585676-7ad7ee00-81b4-11eb-8564-ed6fb656d312.png" alt="image-20210309102627045"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585677-7b708480-81b4-11eb-8487-e0aa45fc0567.png" alt="image-20210309102645821"></li>
</ul>
<h2 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h2><ul>
<li><p>FCN기반</p>
</li>
<li><p>낮은 레이어와 높은 레이어에 있는 결과를 더 잘 결합하는 방법을 제시함</p>
</li>
<li>Contracting Path<ul>
<li>풀링하면서 receptive field를 크게 확보하기 위해 해상도를 낮추고 채널수를 늘림</li>
</ul>
</li>
<li><p>Expanding Path</p>
<ul>
<li>채널 사이즈가 점점 줄어들지만 해상도는 늘어남</li>
<li>대칭되는 Contracting path의 layer에서 skip connection을 통해 대칭되는 feature map들을 가져와서 concat</li>
<li>한번에 Upsampling 하지 않고 차례대로 단계적으로 해상도를 올려줌</li>
</ul>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110585680-7c091b00-81b4-11eb-8df0-b7deecea8cb0.png" alt="image-20210309102718011"></p>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585681-7c091b00-81b4-11eb-8ec9-e7e2bb327e2e.png" alt="image-20210309102801471"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585683-7ca1b180-81b4-11eb-92e8-26e681db668e.png" alt="image-20210309102812247"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585687-7ca1b180-81b4-11eb-9b00-0d8177c03100.png" alt="image-20210309102900873"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585690-7d3a4800-81b4-11eb-8f69-e8b0d1f0863a.png" alt="image-20210309102912950"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585693-7dd2de80-81b4-11eb-9463-d10cfa6fa7c7.png" alt="image-20210309102925369"></li>
<li>이미지 사이즈를 모두 짝수로 유지해야 함<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110585695-7dd2de80-81b4-11eb-92e6-7605f09878e4.png" alt="image-20210309102939118"></li>
</ul>
</li>
</ul>
<h2 id="DeepLab"><a href="#DeepLab" class="headerlink" title="DeepLab"></a>DeepLab</h2><ul>
<li>후처리에 CRFs 사용<ul>
<li>픽셀과 픽셀 사이의 관계 이어줌</li>
<li>regular한 pixel map을 그리드로 봄→ 최적화를 통해 경계 잘 찾을 수 있도록 모델링</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585697-7e6b7500-81b4-11eb-89d6-4382ddc15e9d.png" alt="image-20210309104221164"></li>
</ul>
</li>
<li>Atrous convolution<ul>
<li>컨볼루션 필터 사이에 Dilation factor 만큼 일정한 공간을 넣어줌</li>
<li>파라미터 수는 늘리지 않으면서 receptive field는 exponential하게 키울 수 있다</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585699-7f040b80-81b4-11eb-86ee-840b32229dd2.png" alt="image-20210309104357419"></li>
</ul>
</li>
<li>Depthwise separable convolution<ul>
<li>기존의 convolution 연산은 하나의 필터를 모든 input 채널에 대입시켰다.</li>
<li>기본 convolution 연산을 둘로 나눠서 conv 의 표현력을 어느정도 유지하면서 계산량은 획기적으로 줄어듦</li>
<li>채널 별로 conv 해서 각각 값을 뽑음 + 1x1 conv 통해 하나의 값으로 출력되게 만듦</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585700-7f9ca200-81b4-11eb-9155-26bd1eb08655.png" alt="image-20210309104416065"></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585704-80353880-81b4-11eb-8891-7bdb21048ce1.png" alt="image-20210309104555324"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585706-80cdcf00-81b4-11eb-945b-f727edcb2181.png" alt="image-20210309104605246"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110585709-81666580-81b4-11eb-8923-7bb34e41dad7.png" alt="image-20210309104614535"></li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/09/BoostCamp/Day31/"><img class="fill" src="/img/boostcamp.png" alt="Day31"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-09T04:29:29.000Z" title="2021-3-9 1:29:29 ├F10: PM┤">2021-03-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-22T09:32:45.726Z" title="2021-3-22 6:32:45 ├F10: PM┤">2021-03-22</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">15 minutes read (About 2232 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/09/BoostCamp/Day31/">Day31</a></h1><div class="content"><h1 id="Course-overview"><a href="#Course-overview" class="headerlink" title="Course overview"></a>Course overview</h1><h2 id="What-is-computer-vision"><a href="#What-is-computer-vision" class="headerlink" title="What is computer vision?"></a>What is computer vision?</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110418652-4e9c6e80-80db-11eb-88bc-33457d710002.png" alt="image-20210308091617960"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418654-4f350500-80db-11eb-905a-20146863ecc4.png" alt="image-20210308091711863"><ul>
<li>시각적 데이터에서 representation을 추출하는 일을 Inverse Rendering이라고 한다</li>
<li>representation을 통해 장면에 해당하는 이미지나 3D 모델을 재구현하는것을 Computer Graphics, 또는 렌더링(Rendering)이라고 한다.</li>
</ul>
</li>
<li>How to implement?<ul>
<li>머신러닝: feature를 사용자가 직접 지정해주는 작업 필요 =&gt; 딥러닝의 경사하강법을 통한 feature extraction과 대조</li>
<li>딥러닝: 이미지를 입력받아 내부적으로 추상적인 변수를 추출(feature extraction)<ul>
<li>정답을 예측하는 과정에서 feature를 update</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418656-4fcd9b80-80db-11eb-89f5-ee8bb21264b0.png" alt="image-20210308092029375"></li>
</ul>
</li>
</ul>
<h2 id="What-you-will-learn-in-this-course"><a href="#What-you-will-learn-in-this-course" class="headerlink" title="What you will learn in this course"></a>What you will learn in this course</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110418658-4fcd9b80-80db-11eb-8a63-6965c218c280.png" alt="image-20210308092307918"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418661-50fec880-80db-11eb-996f-5b0e4d3076e9.png" alt="image-20210308092341446"></li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418663-50fec880-80db-11eb-9510-7f9c895633de.png" alt="image-20210308092351755"></p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418666-51975f00-80db-11eb-97c3-82a1234f5887.png" alt="image-20210308092413994"></p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418667-522ff580-80db-11eb-936d-9708c5d48f85.png" alt="image-20210308092424390"></p>
</li>
</ul>
<h1 id="Image-classification"><a href="#Image-classification" class="headerlink" title="Image classification"></a>Image classification</h1><h2 id="What-is-classification"><a href="#What-is-classification" class="headerlink" title="What is classification"></a>What is classification</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110418668-522ff580-80db-11eb-9f11-b6a55c452250.png" alt="image-20210308092643818"></li>
</ul>
<h2 id="An-ideal-approach-for-image-recognition"><a href="#An-ideal-approach-for-image-recognition" class="headerlink" title="An ideal approach for image recognition"></a>An ideal approach for image recognition</h2><ul>
<li><p>가장 이상적인 classifier는 세상에 존재하는 <strong>모든 이미지 데이터를 “유사한” 이미지끼리 모아서<code>KNN(K-Nearest Neighbors)</code> 을 적용</strong>하는 것이다. <img src="https://user-images.githubusercontent.com/46857207/110418669-52c88c00-80db-11eb-991f-430cb0c1e390.png" alt="image-20210308092742426"></p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418671-53612280-80db-11eb-91c4-43c468161e7f.png" alt="image-20210308092943271"></p>
</li>
<li>영상간 유사도를 정의하는 것도 쉬운일이 아님</li>
</ul>
<h2 id="NN-vs-CNN"><a href="#NN-vs-CNN" class="headerlink" title="NN vs CNN"></a>NN vs CNN</h2><ul>
<li><p>NN의 가장 큰 문제점은 <strong>이미지 전체의 패턴에 대해 학습</strong>했기 때문에, 가령 <strong>반쯤 잘리거나 학습된 이미지의 패턴과는 전혀 다른 이미지가 주어진다면 올바른 결과를 내지 못하는 것에 있다.</strong> 또한 <strong>이미지의 크기가 커진다면 학습해야할 파라미터의 수가 증가</strong>한다</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418673-53612280-80db-11eb-9e25-b38d17b2fabf.png" alt="image-20210308093038831"></p>
</li>
<li>CNN 은 Fully-connected layer가 아닌 Locally-connected layer 를 통해 <strong>local feature들을 학습</strong>하게 하고, <strong>파라미터를 공유(shared parameter)</strong> 함으로써 <strong>학습해야 할 파라미터의 수를 줄일수 있도록</strong> 디자인 하였다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418675-53f9b900-80db-11eb-9625-556ce9a3b175.png" alt="image-20210308093513900"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418677-53f9b900-80db-11eb-84fa-de40fb2c3e96.png" alt="image-20210308093714664"></li>
</ul>
<h1 id="CNN-architectures-for-image-classification1"><a href="#CNN-architectures-for-image-classification1" class="headerlink" title="CNN architectures for image classification1"></a>CNN architectures for image classification1</h1><h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><ul>
<li>2012년 ILSVRC에서 1위를 차지한 모델</li>
<li>합성곱 연산과 풀링 연산이 반복되는 구조<ul>
<li>합성곱 필터 크기/stride: 11×11, 5×5, 3×3 / 1<ul>
<li>이전의 LeNet보다 크기가 큰 이미지를 입력받아, 더 큰 필터를 사용</li>
</ul>
</li>
<li>풀링 필터 크기/stride: 2 × 2 / 2</li>
</ul>
</li>
<li>7개의 레이어, 605K개의 노드, 60M개의 파라미터</li>
<li>1.2M개의 학습 데이터를 활용</li>
<li><p>ReLU, Dropout 활용</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418678-54924f80-80db-11eb-98ad-596d4933d447.png" alt="image-20210308093857600"></p>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418682-552ae600-80db-11eb-9d9c-77128ca3cfef.png" alt="image-20210308094240009"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418683-552ae600-80db-11eb-9aeb-037723918c7a.png" alt="image-20210308094352038"><ul>
<li>명암을 normalization</li>
<li>지금은 사용 안함</li>
<li>대신 batch normalization 사용</li>
</ul>
</li>
<li>Receptive field<ul>
<li>입력된 이미지 일부가 합성곱/풀링 과정을 거쳐 한 픽셀로 맵핑되었을 떄, 일부 입력의 크기를 의미</li>
<li>즉, 각 필터가 입력 이미지의 어느 부분만큼 인식하는 지를 의미</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418686-55c37c80-80db-11eb-8b30-81cd37cf6ae4.png" alt="image-20210308094625467"></li>
</ul>
</li>
</ul>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110418690-578d4000-80db-11eb-8ff1-306cc0becdb5.png" alt="image-20210308094708282"><ul>
<li>깊은 레이어</li>
<li>no local response normalization</li>
<li>only 3x3 filter, 2x2 max pool<ul>
<li>커널 사이즈가 커지면 receptive field 가 커지고, 그만큼 많은 영역의 정보를 파악할 수 있다. 반면 학습해야하는 parameter의 수가 커지는 문제점이 있다</li>
<li><strong><code>5x5</code> receptive field 는 두개의 <code>3x3</code> 커널을 이용하는 것 과 같으면서 parameter의 수는 줄어든다</strong></li>
</ul>
</li>
<li>better performance</li>
<li>better generalization<ul>
<li>다른 task적용</li>
</ul>
</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418693-5825d680-80db-11eb-8b1d-e402307d8ab2.png" alt="image-20210308094900090"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418696-59570380-80db-11eb-8adf-281dff9c524b.png" alt="image-20210308094945220"><ul>
<li>작은 커널 사이즈로도 깊게 쌓으면 큰 receptive field를 얻을 수 있다-&gt; 이미지 많은 부븐을 고려 할 수 있다</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418698-59570380-80db-11eb-8060-b1e8c16cbdc1.png" alt="image-20210308095117725"></li>
</ul>
<h1 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h1><h2 id="Learning-representation-of-dataset"><a href="#Learning-representation-of-dataset" class="headerlink" title="Learning representation of dataset"></a>Learning representation of dataset</h2><ul>
<li>Dataset is (almost) always biased<ul>
<li>Images taken by camera(trainingdata)≠ realdata</li>
</ul>
</li>
<li>양질의 이미지를 얻기는 어렵고 고비용 =&gt; 기존의 이미지를 변형하여 학습 데이터로 활용</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418699-59ef9a00-80db-11eb-8436-353f7653e5b1.png" alt="image-20210308100103147"></li>
<li>Augmenting data to fill more space and to close the gap</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418703-5a883080-80db-11eb-9659-4543ea7380de.png" alt="image-20210308100420379"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418705-5a883080-80db-11eb-846d-672cf018efd4.png" alt="image-20210308100431686"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418707-5b20c700-80db-11eb-8b88-2ee5de06816d.png" alt="image-20210308100451092"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418710-5bb95d80-80db-11eb-8caf-53b73c457f8b.png" alt="image-20210308100509050"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418712-5c51f400-80db-11eb-8c17-ca15a2f13c92.png" alt="image-20210308100915719"></li>
<li>어떤 aug사용?</li>
<li>많은 augmentation 기법이 존재하지만 최적의 기법을 찾는것은 어렵다. 또한 한번의 augmentation이 아닌 <strong>일련의 augmentation(Policy)</strong> 을 수행할 필요도 있다.</li>
<li>RandAugment는 자동으로 <strong>최적의 policy를 찾아 어느정도의 강도로 적용할지 찾는 것을 목표</strong>로 한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418714-5c51f400-80db-11eb-912e-e02654591272.png" alt="image-20210308101030019"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418717-5cea8a80-80db-11eb-8497-065f6b90784b.png" alt="image-20210308101107163"></li>
</ul>
<h1 id="Leveraging-pre-trained-information"><a href="#Leveraging-pre-trained-information" class="headerlink" title="Leveraging pre-trained information"></a>Leveraging pre-trained information</h1><h2 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h2><ul>
<li>The high-quality dataset is expensive and hard to obtain</li>
<li>Knowledge learned from one dataset can be applied to other datasets!</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418719-5cea8a80-80db-11eb-9efb-c8da6c3d2c0b.png" alt="image-20210308101518481"><ul>
<li>Pre-trained model에서 기존의 FC layer를 target task에 맞는 FC layer를 적용하여 pretrained model의 convolution layer 들의 가중치는 freeze 하고 FC layer의 가중치만 학습을 하는 방법이다. 따라서 <strong>학습데이터셋이 적더라도 적은 파라미터만 학습 시키기 때문에 효율적으로 학습</strong>시킬 수 있다.</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418721-5e1bb780-80db-11eb-96d0-b0205b5a522b.png" alt="image-20210308101543371"><ul>
<li><strong>pretrained model 의 convolution layer도 같이 학습</strong>하게 하는데, <strong>convolution layer 의 learning rate 는 낮게</strong> / <strong>target task를 위한 FC layer 는 높게 학습</strong> 하도록 하여 target task 에 빠르게 적응하도록 한다. 따라서 위의 방법보단 더 많은 파라미터를 학습시키기 때문에 조금 더 많은 데이터셋이 필요할 수 있다.</li>
</ul>
</li>
</ul>
<h2 id="Knowledge-distillation"><a href="#Knowledge-distillation" class="headerlink" title="Knowledge distillation"></a>Knowledge distillation</h2><ul>
<li><strong>pretrained model(Teacher Model) 의 학습된 지식을 더 작은 model(Student Model)로 지식을 전달하여 모델을 압축</strong>하는 방법이다.<strong><code>KL-divergence loss</code> 를 통해 teacher model의 output distribution과 student model 의 output distribution 이 유사해지도록 학습</strong>을 한다. 이때, <strong>student model의 데이터셋이 없다면 unsupervised-learning</strong> 으로 진행되어 <strong>student model 만을 업데이트</strong>하게 된다</li>
<li><p>teacher model의 출력 label(예측)을 ground truth 레이블인 것 처럼 소형 모델에게 학습시키는 <code>pseudo-labeling</code> 방식으로도 사용되고 있다.</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418723-5e1bb780-80db-11eb-8f1a-6a08fbb7f689.png" alt="image-20210308101727950"></p>
</li>
<li>레이블을 활용하지 않는 경우<ul>
<li>Student 모델이 Teacher 모델의 Inference와 가까워지도록 학습</li>
<li>KL-div</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418841-873c4800-80db-11eb-85c5-603ca7162ac0.png" alt="image-20210309132604257"></li>
</ul>
</li>
<li>레이블을 활용할 경우<ul>
<li>Teacher 모델의 Inference와 Ground Truth를 모두 참고하여 학습. 즉, Teacher 모델을 모사함과 동시에 정확성을 높이는 셈</li>
<li>Teacher 모델의 Inference와의 괴리와 Ground Truth와의 괴리를 가중합한 Loss를 바탕으로 역전파, Student 모델을 업데이트</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418838-86a3b180-80db-11eb-89b9-e5b0055c7a5c.png" alt="image-20210309132509440"></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418729-5f4ce480-80db-11eb-858d-139ee0b4ce9c.png" alt="image-20210308102055943"></li>
<li><code>softmax with temperature T(Soft Prediction)</code>의 경우 <strong>출력의 값을 smooth 하게 만들어주는 기능</strong> 을 한다</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418843-87d4de80-80db-11eb-9071-cab62feb6bed.png" alt="image-20210309132632857"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418732-6116a800-80db-11eb-9253-bfcc0f36b2ac.png" alt="image-20210308102436978"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418735-62e06b80-80db-11eb-83d4-203e210b5368.png" alt="image-20210308102624212"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110430442-c1afe000-80ef-11eb-8b2c-04f33a81d3f8.png" alt="image-20210309155414114"></li>
</ul>
<h1 id="Leveraging-unlabeled-dataset-for-training"><a href="#Leveraging-unlabeled-dataset-for-training" class="headerlink" title="Leveraging unlabeled dataset for training"></a>Leveraging unlabeled dataset for training</h1><p>일반적으로 <strong>많은 데이터들은 unlabeled data 이며 labeled data 는 극히 일부분</strong>이다. 그렇다면 <strong>unlabeled data를 활용하여 학습할 수 있는 방법</strong>은 없을까?</p>
<h2 id="Semi-supervised-learning"><a href="#Semi-supervised-learning" class="headerlink" title="Semi-supervised learning"></a>Semi-supervised learning</h2><ul>
<li><strong>labeled data를 활용하여 학습된 pretrained model</strong> 로 <strong>unlabeled data를 예측하여 pseudo-labeled data를 생성</strong>하고 <strong>labeled data 와 pseudo-labeled data를 활용하여 pretrained model 또는 새로운 model을 재학습</strong>한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418760-6ecc2d80-80db-11eb-9c48-a38812782ef3.png" alt="image-20210308102830818"></li>
</ul>
<h2 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h2><ul>
<li>Augmentation + Teacher-Student Network + Semi-supervised Learning</li>
<li>사전학습된 Teacher Model을 통해 Pseudo Labeling을 진행, 사용 가능한 모든 데이터를 통해 Student Model을 학습</li>
<li>Knowledge Distillation의 학습 방식과 같이, Ground Truth와 Teacher Model의 Inference를 모두 고려하여 학습</li>
<li><p>Student 모델이 Teacher Model의 성능을 넘을 경우, 해당 모델을 Teacher Model로 대체. 다시 위 과정을 반복</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418776-755aa500-80db-11eb-960a-6bebd52542fe.png" alt="image-20210308102910633"></p>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418784-78ee2c00-80db-11eb-9c6c-b414d20b1642.png" alt="image-20210308103040541"><ul>
<li>student model이 계속 커짐</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418786-78ee2c00-80db-11eb-8e24-3e353cf0a391.png" alt="image-20210308103128975"></li>
</ul>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/05/BoostCamp/Day30/"><img class="fill" src="/img/boostcamp.png" alt="Day30"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-05T06:35:03.000Z" title="2021-3-5 3:35:03 ├F10: PM┤">2021-03-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-22T09:32:40.179Z" title="2021-3-22 6:32:40 ├F10: PM┤">2021-03-22</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">a few seconds read (About 6 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/05/BoostCamp/Day30/">Day30</a></h1><div class="content"><h1 id="AI-ML-퀀트"><a href="#AI-ML-퀀트" class="headerlink" title="AI/ML 퀀트"></a>AI/ML 퀀트</h1><h1 id="AI-Ethics"><a href="#AI-Ethics" class="headerlink" title="AI Ethics"></a>AI Ethics</h1></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/05/BoostCamp/Day29/"><img class="fill" src="/img/boostcamp.png" alt="Day29"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-05T06:34:42.000Z" title="2021-3-5 3:34:42 ├F10: PM┤">2021-03-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-22T09:32:36.097Z" title="2021-3-22 6:32:36 ├F10: PM┤">2021-03-22</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">a few seconds read (About 21 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/05/BoostCamp/Day29/">Day29</a></h1><div class="content"><h1 id="언어-모델링"><a href="#언어-모델링" class="headerlink" title="언어 모델링"></a>언어 모델링</h1><h1 id="내가-만든-AI-모델은-합법일까-불법일까"><a href="#내가-만든-AI-모델은-합법일까-불법일까" class="headerlink" title="내가 만든 AI 모델은 합법일까, 불법일까"></a>내가 만든 AI 모델은 합법일까, 불법일까</h1></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/03/BoostCamp/Day28/"><img class="fill" src="/img/boostcamp.png" alt="Day28"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-03T06:30:12.000Z" title="2021-3-3 3:30:12 ├F10: PM┤">2021-03-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-22T09:32:29.892Z" title="2021-3-22 6:32:29 ├F10: PM┤">2021-03-22</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">a few seconds read (About 31 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/03/BoostCamp/Day28/">Day28</a></h1><div class="content"><h1 id="캐글-경진대회-노하우"><a href="#캐글-경진대회-노하우" class="headerlink" title="캐글 경진대회 노하우"></a>캐글 경진대회 노하우</h1><ul>
<li>자신만의 파이프라인 구축</li>
<li>notebooks탭 참고</li>
<li>stratified k fold</li>
</ul>
<h2 id="Full-stack-ML-Engineer"><a href="#Full-stack-ML-Engineer" class="headerlink" title="Full stack ML Engineer"></a>Full stack ML Engineer</h2></div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2021/03/03/BoostCamp/Day27/"><img class="fill" src="/img/boostcamp.png" alt="Day27"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-03T06:30:04.000Z" title="2021-3-3 3:30:04 ├F10: PM┤">2021-03-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-22T09:32:22.423Z" title="2021-3-22 6:32:22 ├F10: PM┤">2021-03-22</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">7 minutes read (About 999 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/03/BoostCamp/Day27/">Day27</a></h1><div class="content"><h1 id="서비스-향-AI-모델-개발-VS-수업-학교-연구-AI-모델-개발"><a href="#서비스-향-AI-모델-개발-VS-수업-학교-연구-AI-모델-개발" class="headerlink" title="서비스 향 AI 모델 개발 VS 수업/학교/연구 AI 모델 개발"></a>서비스 향 AI 모델 개발 VS 수업/학교/연구 AI 모델 개발</h1><h2 id="연구-관점에서-AI-개발이란"><a href="#연구-관점에서-AI-개발이란" class="headerlink" title="연구 관점에서 AI 개발이란?"></a>연구 관점에서 AI 개발이란?</h2><ul>
<li>보통 수업/학교/연구에서는 정해진 데이터셋/평가 방식에서 더 좋은 모델을 찾는 일을 한다</li>
</ul>
<h2 id="서비스-관점에서-AI-개발이란"><a href="#서비스-관점에서-AI-개발이란" class="headerlink" title="서비스 관점에서 AI 개발이란?"></a>서비스 관점에서 AI 개발이란?</h2><ul>
<li>서비스 개발 시에는 학습 데이터셋도 없고, 테스트 데이터셋과 테스트 방법도 없다.</li>
<li>서비스 개발 시에는 서비스 요구 사항만이 있다.</li>
<li>그래서, 첫 번째로 해야 할 일은 학습 데이터셋을 준비하는 것이다.</li>
<li>정확히는 서비스 요구사항으로 부터 학습 데이터셋의 종류/수량/정답을 정해야 한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763297-3e951280-7c35-11eb-9c1a-3a359fd840b4.png" alt="image-20210302091752781"></li>
<li>지금까지의 이야기를 종합하면, 다음과 같은 입출력을 갖는 기술 모듈을 개발해 달라는 요청</li>
<li>결국 학습 데이터 준비를 하려면 모델 파이프 라인 설계가 되어 있어야 한다!</li>
<li>그런데, 모델 파이프 라인 설계 하려면 어느 정도 데이터가 있어야 한다!</li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763304-3fc63f80-7c35-11eb-8747-46fe2b19d5d1.png" alt="image-20210302092917232"></li>
</ul>
<h2 id="자-본인이-학습-데이터셋-준비-담당자라고-해보고-어떤-일을-겪게-되는지-살펴보자"><a href="#자-본인이-학습-데이터셋-준비-담당자라고-해보고-어떤-일을-겪게-되는지-살펴보자" class="headerlink" title="자! 본인이 학습 데이터셋 준비 담당자라고 해보고, 어떤 일을 겪게 되는지 살펴보자."></a>자! 본인이 학습 데이터셋 준비 담당자라고 해보고, 어떤 일을 겪게 되는지 살펴보자.</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/109763307-3fc63f80-7c35-11eb-82b8-82437cbde0c7.png" alt="image-20210302093238647"></li>
<li>다시 한 번 정리해 보면…</li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763309-405ed600-7c35-11eb-9b5f-72ab272f119e.png" alt="image-20210302093255134"></li>
<li>테스트 데이터셋은 학습 데이터셋에서 일부 사용한다고 하고, (사실은 이것도 할 얘기가 많지만..) 서비스 요구사항으로부터 테스트 방법을 도출해야 한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763311-40f76c80-7c35-11eb-8754-3291b67918d2.png" alt="image-20210302093355918"></li>
<li>테스트 방법에 대해 다음처럼 정리할 수 있다.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/109763313-40f76c80-7c35-11eb-9b22-f5e5762c1cee.png" alt="image-20210302093751870"></li>
</ul>
</li>
</ul>
<h2 id="추가로-모델에-관련한-요구사항을-도출해야-합니다"><a href="#추가로-모델에-관련한-요구사항을-도출해야-합니다" class="headerlink" title="추가로, 모델에 관련한 요구사항을 도출해야 합니다."></a>추가로, 모델에 관련한 요구사항을 도출해야 합니다.</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/109763315-41900300-7c35-11eb-8236-fab2d117d4e6.png" alt="image-20210302094115890"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763317-41900300-7c35-11eb-802f-3591b1b0efad.png" alt="image-20210302094318205"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763318-42289980-7c35-11eb-80dd-2f4532f06717.png" alt="image-20210302094455960"></li>
</ul>
<h2 id="서비스-향-AI-모델-개발-기술팀의-조직-구성"><a href="#서비스-향-AI-모델-개발-기술팀의-조직-구성" class="headerlink" title="서비스 향 AI 모델 개발 기술팀의 조직 구성"></a>서비스 향 AI 모델 개발 기술팀의 조직 구성</h2><ul>
<li>AI 기술팀에게는 서비스 요구사항이 오고, 이에 맞는 AI 모델을 개발해야 한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763320-42289980-7c35-11eb-8295-4a4f378a67a5.png" alt="image-20210302094808974"></li>
<li>그런데, 기술팀에 AI 모델 Serving까지 요구되면 필요한 인력은 늘어난다.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/109763322-42c13000-7c35-11eb-8854-54c5779b0e74.png" alt="image-20210302095004548"></li>
</ul>
</li>
<li>마지막으로 모델을 실제 서빙하기 위한 추가 작업들이 end device에 맞춰 더 있다.<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/109763325-4359c680-7c35-11eb-94c6-cf5df1f6fb73.png" alt="image-20210302095037789"></li>
</ul>
</li>
</ul>
<h2 id="AI쪽으로-커리어를-쌓고자-하시는-분들에게-드리고-싶은-말씀"><a href="#AI쪽으로-커리어를-쌓고자-하시는-분들에게-드리고-싶은-말씀" class="headerlink" title="AI쪽으로 커리어를 쌓고자 하시는 분들에게 드리고 싶은 말씀"></a>AI쪽으로 커리어를 쌓고자 하시는 분들에게 드리고 싶은 말씀</h2><ul>
<li>개발자 ⇒ AI 관련 전환<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/109763327-4359c680-7c35-11eb-81ff-6e44aca10562.png" alt="image-20210302095147607"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763329-43f25d00-7c35-11eb-8cd5-e27657667bda.png" alt="image-20210302095203154"></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763331-43f25d00-7c35-11eb-9b59-c1b0ab35e6c9.png" alt="image-20210302095225118"></li>
</ul>
<hr>
<h1 id="AI-시대의-커리어-빌딩"><a href="#AI-시대의-커리어-빌딩" class="headerlink" title="AI 시대의 커리어 빌딩"></a>AI 시대의 커리어 빌딩</h1><h2 id="Careers-in-AI"><a href="#Careers-in-AI" class="headerlink" title="Careers in AI"></a>Careers in AI</h2><ul>
<li>학교를 가야하나요? 회사를 가야하나요?</li>
<li>AI를 다루는 회사의 종류<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/109763334-448af380-7c35-11eb-97ca-bc99a14ca448.png" alt="image-20210302095852320"></li>
</ul>
</li>
<li>AI를 다루는 팀의 구성<ul>
<li><img src="https://user-images.githubusercontent.com/46857207/109763336-448af380-7c35-11eb-91e3-27f52bd75a3e.png" alt="image-20210302095936606"></li>
</ul>
</li>
<li>AI 팀에서 엔지니어가 되면 어떤 일을 할까요? 보통 논문 읽고 모델 학습하는 일을 떠올리는 분들이 많습니다만…</li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763339-45238a00-7c35-11eb-9b3d-8083c51830fd.png" alt="image-20210302100038662"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763342-45bc2080-7c35-11eb-94ab-3e570b8ea47a.png" alt="image-20210302100111645"></li>
<li>현실에서는 정말 다양한 역할이 있고 100% 하나의 포지션의 역할을 수행하는 경우는 드묾</li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763345-45bc2080-7c35-11eb-968b-a34616dc949d.png" alt="image-20210302100215142"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/109763348-4654b700-7c35-11eb-8574-864f76b8ed8b.png" alt="image-20210302100651701"></li>
</ul>
<h2 id="How-to-start-my-AI-engineering-career"><a href="#How-to-start-my-AI-engineering-career" class="headerlink" title="How to start my AI engineering career"></a>How to start my AI engineering career</h2><ul>
<li>Understand yourself</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/AI/BoostCamp/">Previous</a></div><div class="pagination-next"><a href="/categories/AI/BoostCamp/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/AI/BoostCamp/">1</a></li><li><a class="pagination-link is-current" href="/categories/AI/BoostCamp/page/2/">2</a></li><li><a class="pagination-link" href="/categories/AI/BoostCamp/page/3/">3</a></li><li><a class="pagination-link" href="/categories/AI/BoostCamp/page/4/">4</a></li><li><a class="pagination-link" href="/categories/AI/BoostCamp/page/5/">5</a></li></ul></nav></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/./img/avatar.jpg" alt="Keonwoo Choi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Keonwoo Choi</p><p class="is-size-6 is-block">blog</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">46</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/KeonwooChoi" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/KeonwooChoi"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">46</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/BoostCamp/"><span class="level-start"><span class="level-item">BoostCamp</span></span><span class="level-end"><span class="level-item tag">45</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/BoostCamp/Project-Stage/"><span class="level-start"><span class="level-item">Project Stage</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/AI/Pytorch/"><span class="level-start"><span class="level-item">Pytorch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-08T05:49:01.000Z">2021-04-08</time></p><p class="title"><a href="/2021/04/08/BoostCamp/Project%20Stage/Day48/">Day48</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a> / <a href="/categories/AI/BoostCamp/Project-Stage/">Project Stage</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-06T06:18:25.000Z">2021-04-06</time></p><p class="title"><a href="/2021/04/06/BoostCamp/Project%20Stage/Day47/">Day47</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a> / <a href="/categories/AI/BoostCamp/Project-Stage/">Project Stage</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-06T06:18:16.000Z">2021-04-06</time></p><p class="title"><a href="/2021/04/06/BoostCamp/Project%20Stage/Day46/">Day46</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a> / <a href="/categories/AI/BoostCamp/Project-Stage/">Project Stage</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-02T15:27:10.000Z">2021-04-03</time></p><p class="title"><a href="/2021/04/03/BoostCamp/Project%20Stage/Day45/">Day45</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a> / <a href="/categories/AI/BoostCamp/Project-Stage/">Project Stage</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-01T15:10:20.000Z">2021-04-02</time></p><p class="title"><a href="/2021/04/02/BoostCamp/Project%20Stage/Day44/">Day44</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/BoostCamp/">BoostCamp</a> / <a href="/categories/AI/BoostCamp/Project-Stage/">Project Stage</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">March 2021</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li></ul></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">blog</a><p class="is-size-7"><span>&copy; 2021 Keonwoo Choi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>