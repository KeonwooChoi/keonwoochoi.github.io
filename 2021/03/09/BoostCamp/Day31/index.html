<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Day31 - blog</title><meta name="robots" content="noindex"><link rel="manifest" href="/manifest.json"><meta name="application-name" content="blog"><meta name="msapplication-TileImage" content="./img/favicon3.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Course overviewWhat is computer vision?   시각적 데이터에서 representation을 추출하는 일을 Inverse Rendering이라고 한다 representation을 통해 장면에 해당하는 이미지나 3D 모델을 재구현하는것을 Computer Graphics, 또는 렌더링(Rendering)이라고 한다.   How to"><meta property="og:type" content="blog"><meta property="og:title" content="Day31"><meta property="og:url" content="https://keonwoochoi.github.io/2021/03/09/BoostCamp/Day31/"><meta property="og:site_name" content="blog"><meta property="og:description" content="Course overviewWhat is computer vision?   시각적 데이터에서 representation을 추출하는 일을 Inverse Rendering이라고 한다 representation을 통해 장면에 해당하는 이미지나 3D 모델을 재구현하는것을 Computer Graphics, 또는 렌더링(Rendering)이라고 한다.   How to"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://keonwoochoi.github.io/img/boostcamp.png"><meta property="article:published_time" content="2021-03-09T04:29:29.000Z"><meta property="article:modified_time" content="2021-03-09T06:54:48.986Z"><meta property="article:author" content="Keonwoo Choi"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/boostcamp.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://keonwoochoi.github.io/2021/03/09/BoostCamp/Day31/"},"headline":"blog","image":["https://keonwoochoi.github.io/img/boostcamp.png"],"datePublished":"2021-03-09T04:29:29.000Z","dateModified":"2021-03-09T06:54:48.986Z","author":{"@type":"Person","name":"Keonwoo Choi"},"description":"Course overviewWhat is computer vision?   시각적 데이터에서 representation을 추출하는 일을 Inverse Rendering이라고 한다 representation을 통해 장면에 해당하는 이미지나 3D 모델을 재구현하는것을 Computer Graphics, 또는 렌더링(Rendering)이라고 한다.   How to"}</script><link rel="canonical" href="https://keonwoochoi.github.io/2021/03/09/BoostCamp/Day31/"><link rel="icon" href="/./img/favicon3.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">blog</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-10-tablet is-10-desktop is-10-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/img/boostcamp.png" alt="Day31"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-09T04:29:29.000Z" title="2021-3-9 1:29:29 ├F10: PM┤">2021-03-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-03-09T06:54:48.986Z" title="2021-3-9 3:54:48 ├F10: PM┤">2021-03-09</time></span><span class="level-item"><a class="link-muted" href="/categories/AI/">AI</a><span> / </span><a class="link-muted" href="/categories/AI/BoostCamp/">BoostCamp</a></span><span class="level-item">15 minutes read (About 2232 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Day31</h1><div class="content"><h1 id="Course-overview"><a href="#Course-overview" class="headerlink" title="Course overview"></a>Course overview</h1><h2 id="What-is-computer-vision"><a href="#What-is-computer-vision" class="headerlink" title="What is computer vision?"></a>What is computer vision?</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110418652-4e9c6e80-80db-11eb-88bc-33457d710002.png" alt="image-20210308091617960"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418654-4f350500-80db-11eb-905a-20146863ecc4.png" alt="image-20210308091711863"><ul>
<li>시각적 데이터에서 representation을 추출하는 일을 Inverse Rendering이라고 한다</li>
<li>representation을 통해 장면에 해당하는 이미지나 3D 모델을 재구현하는것을 Computer Graphics, 또는 렌더링(Rendering)이라고 한다.</li>
</ul>
</li>
<li>How to implement?<ul>
<li>머신러닝: feature를 사용자가 직접 지정해주는 작업 필요 =&gt; 딥러닝의 경사하강법을 통한 feature extraction과 대조</li>
<li>딥러닝: 이미지를 입력받아 내부적으로 추상적인 변수를 추출(feature extraction)<ul>
<li>정답을 예측하는 과정에서 feature를 update</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418656-4fcd9b80-80db-11eb-89f5-ee8bb21264b0.png" alt="image-20210308092029375"></li>
</ul>
</li>
</ul>
<h2 id="What-you-will-learn-in-this-course"><a href="#What-you-will-learn-in-this-course" class="headerlink" title="What you will learn in this course"></a>What you will learn in this course</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110418658-4fcd9b80-80db-11eb-8a63-6965c218c280.png" alt="image-20210308092307918"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418661-50fec880-80db-11eb-996f-5b0e4d3076e9.png" alt="image-20210308092341446"></li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418663-50fec880-80db-11eb-9510-7f9c895633de.png" alt="image-20210308092351755"></p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418666-51975f00-80db-11eb-97c3-82a1234f5887.png" alt="image-20210308092413994"></p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418667-522ff580-80db-11eb-936d-9708c5d48f85.png" alt="image-20210308092424390"></p>
</li>
</ul>
<h1 id="Image-classification"><a href="#Image-classification" class="headerlink" title="Image classification"></a>Image classification</h1><h2 id="What-is-classification"><a href="#What-is-classification" class="headerlink" title="What is classification"></a>What is classification</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110418668-522ff580-80db-11eb-9f11-b6a55c452250.png" alt="image-20210308092643818"></li>
</ul>
<h2 id="An-ideal-approach-for-image-recognition"><a href="#An-ideal-approach-for-image-recognition" class="headerlink" title="An ideal approach for image recognition"></a>An ideal approach for image recognition</h2><ul>
<li><p>가장 이상적인 classifier는 세상에 존재하는 <strong>모든 이미지 데이터를 “유사한” 이미지끼리 모아서<code>KNN(K-Nearest Neighbors)</code> 을 적용</strong>하는 것이다. <img src="https://user-images.githubusercontent.com/46857207/110418669-52c88c00-80db-11eb-991f-430cb0c1e390.png" alt="image-20210308092742426"></p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418671-53612280-80db-11eb-91c4-43c468161e7f.png" alt="image-20210308092943271"></p>
</li>
<li>영상간 유사도를 정의하는 것도 쉬운일이 아님</li>
</ul>
<h2 id="NN-vs-CNN"><a href="#NN-vs-CNN" class="headerlink" title="NN vs CNN"></a>NN vs CNN</h2><ul>
<li><p>NN의 가장 큰 문제점은 <strong>이미지 전체의 패턴에 대해 학습</strong>했기 때문에, 가령 <strong>반쯤 잘리거나 학습된 이미지의 패턴과는 전혀 다른 이미지가 주어진다면 올바른 결과를 내지 못하는 것에 있다.</strong> 또한 <strong>이미지의 크기가 커진다면 학습해야할 파라미터의 수가 증가</strong>한다</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418673-53612280-80db-11eb-9e25-b38d17b2fabf.png" alt="image-20210308093038831"></p>
</li>
<li>CNN 은 Fully-connected layer가 아닌 Locally-connected layer 를 통해 <strong>local feature들을 학습</strong>하게 하고, <strong>파라미터를 공유(shared parameter)</strong> 함으로써 <strong>학습해야 할 파라미터의 수를 줄일수 있도록</strong> 디자인 하였다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418675-53f9b900-80db-11eb-9625-556ce9a3b175.png" alt="image-20210308093513900"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418677-53f9b900-80db-11eb-84fa-de40fb2c3e96.png" alt="image-20210308093714664"></li>
</ul>
<h1 id="CNN-architectures-for-image-classification1"><a href="#CNN-architectures-for-image-classification1" class="headerlink" title="CNN architectures for image classification1"></a>CNN architectures for image classification1</h1><h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><ul>
<li>2012년 ILSVRC에서 1위를 차지한 모델</li>
<li>합성곱 연산과 풀링 연산이 반복되는 구조<ul>
<li>합성곱 필터 크기/stride: 11×11, 5×5, 3×3 / 1<ul>
<li>이전의 LeNet보다 크기가 큰 이미지를 입력받아, 더 큰 필터를 사용</li>
</ul>
</li>
<li>풀링 필터 크기/stride: 2 × 2 / 2</li>
</ul>
</li>
<li>7개의 레이어, 605K개의 노드, 60M개의 파라미터</li>
<li>1.2M개의 학습 데이터를 활용</li>
<li><p>ReLU, Dropout 활용</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418678-54924f80-80db-11eb-98ad-596d4933d447.png" alt="image-20210308093857600"></p>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418682-552ae600-80db-11eb-9d9c-77128ca3cfef.png" alt="image-20210308094240009"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418683-552ae600-80db-11eb-9aeb-037723918c7a.png" alt="image-20210308094352038"><ul>
<li>명암을 normalization</li>
<li>지금은 사용 안함</li>
<li>대신 batch normalization 사용</li>
</ul>
</li>
<li>Receptive field<ul>
<li>입력된 이미지 일부가 합성곱/풀링 과정을 거쳐 한 픽셀로 맵핑되었을 떄, 일부 입력의 크기를 의미</li>
<li>즉, 각 필터가 입력 이미지의 어느 부분만큼 인식하는 지를 의미</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418686-55c37c80-80db-11eb-8b30-81cd37cf6ae4.png" alt="image-20210308094625467"></li>
</ul>
</li>
</ul>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><ul>
<li><img src="https://user-images.githubusercontent.com/46857207/110418690-578d4000-80db-11eb-8ff1-306cc0becdb5.png" alt="image-20210308094708282"><ul>
<li>깊은 레이어</li>
<li>no local response normalization</li>
<li>only 3x3 filter, 2x2 max pool<ul>
<li>커널 사이즈가 커지면 receptive field 가 커지고, 그만큼 많은 영역의 정보를 파악할 수 있다. 반면 학습해야하는 parameter의 수가 커지는 문제점이 있다</li>
<li><strong><code>5x5</code> receptive field 는 두개의 <code>3x3</code> 커널을 이용하는 것 과 같으면서 parameter의 수는 줄어든다</strong></li>
</ul>
</li>
<li>better performance</li>
<li>better generalization<ul>
<li>다른 task적용</li>
</ul>
</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418693-5825d680-80db-11eb-8b1d-e402307d8ab2.png" alt="image-20210308094900090"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418696-59570380-80db-11eb-8adf-281dff9c524b.png" alt="image-20210308094945220"><ul>
<li>작은 커널 사이즈로도 깊게 쌓으면 큰 receptive field를 얻을 수 있다-&gt; 이미지 많은 부븐을 고려 할 수 있다</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418698-59570380-80db-11eb-8060-b1e8c16cbdc1.png" alt="image-20210308095117725"></li>
</ul>
<h1 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h1><h2 id="Learning-representation-of-dataset"><a href="#Learning-representation-of-dataset" class="headerlink" title="Learning representation of dataset"></a>Learning representation of dataset</h2><ul>
<li>Dataset is (almost) always biased<ul>
<li>Images taken by camera(trainingdata)≠ realdata</li>
</ul>
</li>
<li>양질의 이미지를 얻기는 어렵고 고비용 =&gt; 기존의 이미지를 변형하여 학습 데이터로 활용</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418699-59ef9a00-80db-11eb-8436-353f7653e5b1.png" alt="image-20210308100103147"></li>
<li>Augmenting data to fill more space and to close the gap</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418703-5a883080-80db-11eb-9659-4543ea7380de.png" alt="image-20210308100420379"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418705-5a883080-80db-11eb-846d-672cf018efd4.png" alt="image-20210308100431686"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418707-5b20c700-80db-11eb-8b88-2ee5de06816d.png" alt="image-20210308100451092"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418710-5bb95d80-80db-11eb-8caf-53b73c457f8b.png" alt="image-20210308100509050"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418712-5c51f400-80db-11eb-8c17-ca15a2f13c92.png" alt="image-20210308100915719"></li>
<li>어떤 aug사용?</li>
<li>많은 augmentation 기법이 존재하지만 최적의 기법을 찾는것은 어렵다. 또한 한번의 augmentation이 아닌 <strong>일련의 augmentation(Policy)</strong> 을 수행할 필요도 있다.</li>
<li>RandAugment는 자동으로 <strong>최적의 policy를 찾아 어느정도의 강도로 적용할지 찾는 것을 목표</strong>로 한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418714-5c51f400-80db-11eb-912e-e02654591272.png" alt="image-20210308101030019"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418717-5cea8a80-80db-11eb-8497-065f6b90784b.png" alt="image-20210308101107163"></li>
</ul>
<h1 id="Leveraging-pre-trained-information"><a href="#Leveraging-pre-trained-information" class="headerlink" title="Leveraging pre-trained information"></a>Leveraging pre-trained information</h1><h2 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h2><ul>
<li>The high-quality dataset is expensive and hard to obtain</li>
<li>Knowledge learned from one dataset can be applied to other datasets!</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418719-5cea8a80-80db-11eb-9efb-c8da6c3d2c0b.png" alt="image-20210308101518481"><ul>
<li>Pre-trained model에서 기존의 FC layer를 target task에 맞는 FC layer를 적용하여 pretrained model의 convolution layer 들의 가중치는 freeze 하고 FC layer의 가중치만 학습을 하는 방법이다. 따라서 <strong>학습데이터셋이 적더라도 적은 파라미터만 학습 시키기 때문에 효율적으로 학습</strong>시킬 수 있다.</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418721-5e1bb780-80db-11eb-96d0-b0205b5a522b.png" alt="image-20210308101543371"><ul>
<li><strong>pretrained model 의 convolution layer도 같이 학습</strong>하게 하는데, <strong>convolution layer 의 learning rate 는 낮게</strong> / <strong>target task를 위한 FC layer 는 높게 학습</strong> 하도록 하여 target task 에 빠르게 적응하도록 한다. 따라서 위의 방법보단 더 많은 파라미터를 학습시키기 때문에 조금 더 많은 데이터셋이 필요할 수 있다.</li>
</ul>
</li>
</ul>
<h2 id="Knowledge-distillation"><a href="#Knowledge-distillation" class="headerlink" title="Knowledge distillation"></a>Knowledge distillation</h2><ul>
<li><strong>pretrained model(Teacher Model) 의 학습된 지식을 더 작은 model(Student Model)로 지식을 전달하여 모델을 압축</strong>하는 방법이다.<strong><code>KL-divergence loss</code> 를 통해 teacher model의 output distribution과 student model 의 output distribution 이 유사해지도록 학습</strong>을 한다. 이때, <strong>student model의 데이터셋이 없다면 unsupervised-learning</strong> 으로 진행되어 <strong>student model 만을 업데이트</strong>하게 된다</li>
<li><p>teacher model의 출력 label(예측)을 ground truth 레이블인 것 처럼 소형 모델에게 학습시키는 <code>pseudo-labeling</code> 방식으로도 사용되고 있다.</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418723-5e1bb780-80db-11eb-8f1a-6a08fbb7f689.png" alt="image-20210308101727950"></p>
</li>
<li>레이블을 활용하지 않는 경우<ul>
<li>Student 모델이 Teacher 모델의 Inference와 가까워지도록 학습</li>
<li>KL-div</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418841-873c4800-80db-11eb-85c5-603ca7162ac0.png" alt="image-20210309132604257"></li>
</ul>
</li>
<li>레이블을 활용할 경우<ul>
<li>Teacher 모델의 Inference와 Ground Truth를 모두 참고하여 학습. 즉, Teacher 모델을 모사함과 동시에 정확성을 높이는 셈</li>
<li>Teacher 모델의 Inference와의 괴리와 Ground Truth와의 괴리를 가중합한 Loss를 바탕으로 역전파, Student 모델을 업데이트</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418838-86a3b180-80db-11eb-89b9-e5b0055c7a5c.png" alt="image-20210309132509440"></li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418729-5f4ce480-80db-11eb-858d-139ee0b4ce9c.png" alt="image-20210308102055943"></li>
<li><code>softmax with temperature T(Soft Prediction)</code>의 경우 <strong>출력의 값을 smooth 하게 만들어주는 기능</strong> 을 한다</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418843-87d4de80-80db-11eb-9071-cab62feb6bed.png" alt="image-20210309132632857"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418732-6116a800-80db-11eb-9253-bfcc0f36b2ac.png" alt="image-20210308102436978"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418735-62e06b80-80db-11eb-83d4-203e210b5368.png" alt="image-20210308102624212"></li>
<li><img src="https://user-images.githubusercontent.com/46857207/110430442-c1afe000-80ef-11eb-8b2c-04f33a81d3f8.png" alt="image-20210309155414114"></li>
</ul>
<h1 id="Leveraging-unlabeled-dataset-for-training"><a href="#Leveraging-unlabeled-dataset-for-training" class="headerlink" title="Leveraging unlabeled dataset for training"></a>Leveraging unlabeled dataset for training</h1><p>일반적으로 <strong>많은 데이터들은 unlabeled data 이며 labeled data 는 극히 일부분</strong>이다. 그렇다면 <strong>unlabeled data를 활용하여 학습할 수 있는 방법</strong>은 없을까?</p>
<h2 id="Semi-supervised-learning"><a href="#Semi-supervised-learning" class="headerlink" title="Semi-supervised learning"></a>Semi-supervised learning</h2><ul>
<li><strong>labeled data를 활용하여 학습된 pretrained model</strong> 로 <strong>unlabeled data를 예측하여 pseudo-labeled data를 생성</strong>하고 <strong>labeled data 와 pseudo-labeled data를 활용하여 pretrained model 또는 새로운 model을 재학습</strong>한다.</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418760-6ecc2d80-80db-11eb-9c48-a38812782ef3.png" alt="image-20210308102830818"></li>
</ul>
<h2 id="Self-training"><a href="#Self-training" class="headerlink" title="Self-training"></a>Self-training</h2><ul>
<li>Augmentation + Teacher-Student Network + Semi-supervised Learning</li>
<li>사전학습된 Teacher Model을 통해 Pseudo Labeling을 진행, 사용 가능한 모든 데이터를 통해 Student Model을 학습</li>
<li>Knowledge Distillation의 학습 방식과 같이, Ground Truth와 Teacher Model의 Inference를 모두 고려하여 학습</li>
<li><p>Student 모델이 Teacher Model의 성능을 넘을 경우, 해당 모델을 Teacher Model로 대체. 다시 위 과정을 반복</p>
</li>
<li><p><img src="https://user-images.githubusercontent.com/46857207/110418776-755aa500-80db-11eb-960a-6bebd52542fe.png" alt="image-20210308102910633"></p>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418784-78ee2c00-80db-11eb-9c6c-b414d20b1642.png" alt="image-20210308103040541"><ul>
<li>student model이 계속 커짐</li>
</ul>
</li>
<li><img src="https://user-images.githubusercontent.com/46857207/110418786-78ee2c00-80db-11eb-8e24-3e353cf0a391.png" alt="image-20210308103128975"></li>
</ul>
<!-- flag of hidden posts --></div><div class="article-licensing box"><div class="licensing-title"><p>Day31</p><p><a href="https://keonwoochoi.github.io/2021/03/09/BoostCamp/Day31/">https://keonwoochoi.github.io/2021/03/09/BoostCamp/Day31/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Keonwoo Choi</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-03-09</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-03-09</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><!--!--></article></div><!--!--><!--!--></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/./img/avatar.jpg" alt="Keonwoo Choi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Keonwoo Choi</p><p class="is-size-6 is-block">blog</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Korea</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Post</p><a href="/archives"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/KeonwooChoi" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/KeonwooChoi"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Course-overview"><span class="level-left"><span class="level-item">Course overview</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#What-is-computer-vision"><span class="level-left"><span class="level-item">What is computer vision?</span></span></a></li><li><a class="level is-mobile" href="#What-you-will-learn-in-this-course"><span class="level-left"><span class="level-item">What you will learn in this course</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Image-classification"><span class="level-left"><span class="level-item">Image classification</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#What-is-classification"><span class="level-left"><span class="level-item">What is classification</span></span></a></li><li><a class="level is-mobile" href="#An-ideal-approach-for-image-recognition"><span class="level-left"><span class="level-item">An ideal approach for image recognition</span></span></a></li><li><a class="level is-mobile" href="#NN-vs-CNN"><span class="level-left"><span class="level-item">NN vs CNN</span></span></a></li></ul></li><li><a class="level is-mobile" href="#CNN-architectures-for-image-classification1"><span class="level-left"><span class="level-item">CNN architectures for image classification1</span></span></a></li><li><a class="level is-mobile" href="#AlexNet"><span class="level-left"><span class="level-item">AlexNet</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#VGG"><span class="level-left"><span class="level-item">VGG</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Data-augmentation"><span class="level-left"><span class="level-item">Data augmentation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Learning-representation-of-dataset"><span class="level-left"><span class="level-item">Learning representation of dataset</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Leveraging-pre-trained-information"><span class="level-left"><span class="level-item">Leveraging pre-trained information</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Transfer-learning"><span class="level-left"><span class="level-item">Transfer learning</span></span></a></li><li><a class="level is-mobile" href="#Knowledge-distillation"><span class="level-left"><span class="level-item">Knowledge distillation</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Leveraging-unlabeled-dataset-for-training"><span class="level-left"><span class="level-item">Leveraging unlabeled dataset for training</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Semi-supervised-learning"><span class="level-left"><span class="level-item">Semi-supervised learning</span></span></a></li><li><a class="level is-mobile" href="#Self-training"><span class="level-left"><span class="level-item">Self-training</span></span></a></li></ul></li></ul></div></div><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/AI/Pytorch/"><span class="level-start"><span class="level-item">Pytorch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-25T04:51:41.000Z">2021-01-25</time></p><p class="title"><a href="/2021/01/25/Pytorch/Pytorch1/">Pytorch1</a></p><p class="categories"><a href="/categories/AI/">AI</a> / <a href="/categories/AI/Pytorch/">Pytorch</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">blog</a><p class="is-size-7"><span>&copy; 2021 Keonwoo Choi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>